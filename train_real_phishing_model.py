#!/usr/bin/env python3
"""
åŸºäºçœŸå®é’“é±¼ç½‘ç«™æ•°æ®è®­ç»ƒè¶…å¼ºæ¨¡å‹
"""

import os
import sys
import json
import argparse
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Tuple

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, roc_auc_score, accuracy_score
from loguru import logger
import matplotlib.pyplot as plt

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).resolve().parent
sys.path.insert(0, str(PROJECT_ROOT))

from phishguard_v1.models.fusion_model import FusionDNN
from phishguard_v1.models.training_utils import TrainingArtifacts

class RealPhishingTrainer:
    """çœŸå®é’“é±¼ç½‘ç«™æ¨¡å‹è®­ç»ƒå™¨"""

    def __init__(self,
                 data_dir: str = "data_real_phishing_v2",
                 model_dir: str = "artifacts",
                 device: str = "auto"):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨

        Args:
            data_dir: æ•°æ®é›†ç›®å½•
            model_dir: æ¨¡å‹ä¿å­˜ç›®å½•
            device: è®¾å¤‡ ("auto", "cuda", "cpu")
        """
        self.data_dir = Path(data_dir)
        self.model_dir = Path(model_dir)

        # è®¾å¤‡é€‰æ‹©
        if device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(device)

        logger.info(f"ğŸš€ åˆå§‹åŒ–çœŸå®é’“é±¼ç½‘ç«™æ¨¡å‹è®­ç»ƒå™¨")
        logger.info(f"ğŸ“ æ•°æ®ç›®å½•: {self.data_dir}")
        logger.info(f"ğŸ’¾ æ¨¡å‹ç›®å½•: {self.model_dir}")
        logger.info(f"ğŸ’» è®¡ç®—è®¾å¤‡: {self.device}")

        # åˆ›å»ºæ¨¡å‹ç›®å½•
        self.model_dir.mkdir(exist_ok=True)

        # è®­ç»ƒå†å²
        self.history = []

    def load_data(self) -> Tuple[DataLoader, DataLoader, DataLoader, StandardScaler, Tuple[str, ...]]:
        """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
        logger.info("ğŸ“– åŠ è½½çœŸå®é’“é±¼ç½‘ç«™æ•°æ®é›†...")

        # åŠ è½½æ•°æ®
        train_df = pd.read_parquet(self.data_dir / "train.parquet")
        val_df = pd.read_parquet(self.data_dir / "val.parquet")
        test_df = pd.read_parquet(self.data_dir / "test.parquet")

        logger.info(f"âœ… è®­ç»ƒé›†: {len(train_df)} æ ·æœ¬")
        logger.info(f"âœ… éªŒè¯é›†: {len(val_df)} æ ·æœ¬")
        logger.info(f"âœ… æµ‹è¯•é›†: {len(test_df)} æ ·æœ¬")

        # æ£€æŸ¥æ•°æ®å¹³è¡¡
        logger.info(f"ğŸ“Š è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ: {train_df['label'].value_counts().to_dict()}")
        logger.info(f"ğŸ“Š éªŒè¯é›†æ ‡ç­¾åˆ†å¸ƒ: {val_df['label'].value_counts().to_dict()}")

        # æå–ç‰¹å¾å’Œæ ‡ç­¾
        feature_columns = [col for col in train_df.columns if col != 'label']
        logger.info(f"ğŸ”§ ç‰¹å¾æ•°é‡: {len(feature_columns)}")

        X_train = train_df[feature_columns].values
        y_train = train_df["label"].values.astype(int)

        X_val = val_df[feature_columns].values
        y_val = val_df["label"].values.astype(int)

        X_test = test_df[feature_columns].values
        y_test = test_df["label"].values.astype(int)

        # æ•°æ®æ ‡å‡†åŒ–
        logger.info("ğŸ”„ æ•°æ®æ ‡å‡†åŒ–...")
        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_val = scaler.transform(X_val)
        X_test = scaler.transform(X_test)

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        batch_size = 512 if self.device.type == "cuda" else 256

        train_dataset = TensorDataset(
            torch.tensor(X_train, dtype=torch.float32),
            torch.tensor(y_train, dtype=torch.long)
        )
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)

        val_dataset = TensorDataset(
            torch.tensor(X_val, dtype=torch.float32),
            torch.tensor(y_val, dtype=torch.long)
        )
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)

        test_dataset = TensorDataset(
            torch.tensor(X_test, dtype=torch.float32),
            torch.tensor(y_test, dtype=torch.long)
        )
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)

        return train_loader, val_loader, test_loader, scaler, tuple(feature_columns)

    def create_model(self, num_features: int) -> nn.Module:
        """åˆ›å»ºå¢å¼ºçš„FusionDNNæ¨¡å‹"""
        logger.info(f"ğŸ§  åˆ›å»ºå¢å¼ºFusionDNNæ¨¡å‹ï¼Œç‰¹å¾æ•°: {num_features}")

        # åˆ›å»ºè‡ªå®šä¹‰2åˆ†ç±»æ¨¡å‹
        class EnhancedPhishingDetector(nn.Module):
            def __init__(self, input_dim):
                super().__init__()

                # è¾“å…¥å±‚
                self.fc1 = nn.Linear(input_dim, 1024)
                self.bn1 = nn.BatchNorm1d(1024)
                self.dropout1 = nn.Dropout(0.4)

                # éšè—å±‚
                self.fc2 = nn.Linear(1024, 512)
                self.bn2 = nn.BatchNorm1d(512)
                self.dropout2 = nn.Dropout(0.3)

                self.fc3 = nn.Linear(512, 256)
                self.bn3 = nn.BatchNorm1d(256)
                self.dropout3 = nn.Dropout(0.3)

                self.fc4 = nn.Linear(256, 128)
                self.bn4 = nn.BatchNorm1d(128)
                self.dropout4 = nn.Dropout(0.2)

                self.fc5 = nn.Linear(128, 64)
                self.bn5 = nn.BatchNorm1d(64)
                self.dropout5 = nn.Dropout(0.2)

                self.fc6 = nn.Linear(64, 32)
                self.bn6 = nn.BatchNorm1d(32)
                self.dropout6 = nn.Dropout(0.1)

                # è¾“å‡ºå±‚
                self.fc7 = nn.Linear(32, 2)  # 2åˆ†ç±»ï¼šåˆæ³•vsé’“é±¼

                self._advanced = True

            def forward(self, x):
                x = torch.relu(self.bn1(self.fc1(x)))
                x = self.dropout1(x)

                x = torch.relu(self.bn2(self.fc2(x)))
                x = self.dropout2(x)

                x = torch.relu(self.bn3(self.fc3(x)))
                x = self.dropout3(x)

                x = torch.relu(self.bn4(self.fc4(x)))
                x = self.dropout4(x)

                x = torch.relu(self.bn5(self.fc5(x)))
                x = self.dropout5(x)

                x = torch.relu(self.bn6(self.fc6(x)))
                x = self.dropout6(x)

                x = self.fc7(x)
                return x

        model = EnhancedPhishingDetector(num_features)
        model = model.to(self.device)
        logger.info(f"ğŸ“Š æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

        return model

    def compute_class_weights(self, y_train: np.ndarray) -> torch.Tensor:
        """è®¡ç®—ç±»åˆ«æƒé‡"""
        from sklearn.utils.class_weight import compute_class_weight

        classes = np.unique(y_train)
        weights = compute_class_weight('balanced', classes=classes, y=y_train)
        class_weights = torch.tensor(weights, dtype=torch.float32).to(self.device)

        logger.info(f"âš–ï¸ ç±»åˆ«æƒé‡: {weights}")
        return class_weights

    def train_epoch(self, model: nn.Module, train_loader: DataLoader,
                   optimizer: optim.Optimizer, criterion: nn.Module,
                   class_weights: torch.Tensor) -> Tuple[float, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        model.train()
        total_loss = 0.0
        correct = 0
        total = 0

        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(self.device), target.to(self.device)

            optimizer.zero_grad()
            output = model(data)

            # ä½¿ç”¨åŠ æƒæŸå¤±
            if len(class_weights) == 2:
                loss = criterion(output, target)
                # æ‰‹åŠ¨åº”ç”¨ç±»åˆ«æƒé‡
                weights = class_weights[target]
                loss = (loss * weights).mean()

            loss.backward()

            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()

            total_loss += loss.item()
            pred = output.argmax(dim=1)
            correct += pred.eq(target).sum().item()
            total += target.size(0)

            if batch_idx % 20 == 0:
                logger.info(f"ğŸ“ˆ Batch {batch_idx}/{len(train_loader)} - Loss: {loss.item():.4f}")

        avg_loss = total_loss / len(train_loader)
        accuracy = 100.0 * correct / total

        return avg_loss, accuracy

    def evaluate(self, model: nn.Module, data_loader: DataLoader,
                criterion: nn.Module) -> Tuple[float, float, Dict[str, Any]]:
        """è¯„ä¼°æ¨¡å‹"""
        model.eval()
        total_loss = 0.0
        correct = 0
        total = 0
        all_preds = []
        all_targets = []
        all_probs = []

        with torch.no_grad():
            for data, target in data_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = model(data)
                loss = criterion(output, target)

                total_loss += loss.item()
                pred = output.argmax(dim=1)
                correct += pred.eq(target).sum().item()
                total += target.size(0)

                # æ”¶é›†é¢„æµ‹ç»“æœ
                all_preds.extend(pred.cpu().numpy())
                all_targets.extend(target.cpu().numpy())
                all_probs.extend(torch.softmax(output, dim=1)[:, 1].cpu().numpy())

        avg_loss = total_loss / len(data_loader)
        accuracy = 100.0 * correct / total

        # è®¡ç®—è¯¦ç»†æŒ‡æ ‡
        all_preds = np.array(all_preds)
        all_targets = np.array(all_targets)
        all_probs = np.array(all_probs)

        auc_score = roc_auc_score(all_targets, all_probs)
        report = classification_report(all_targets, all_preds, target_names=['benign', 'phish'], output_dict=True)

        return avg_loss, accuracy, {
            'auc': auc_score,
            'classification_report': report,
            'predictions': all_preds,
            'targets': all_targets,
            'probabilities': all_probs
        }

    def train(self, epochs: int = 100, lr: float = 0.001, patience: int = 15) -> TrainingArtifacts:
        """è®­ç»ƒæ¨¡å‹"""
        logger.info(f"ğŸš€ å¼€å§‹è®­ç»ƒçœŸå®é’“é±¼ç½‘ç«™æ£€æµ‹æ¨¡å‹ - Epochs: {epochs}, LR: {lr}")

        # åŠ è½½æ•°æ®
        train_loader, val_loader, test_loader, scaler, feature_names = self.load_data()

        # åˆ›å»ºæ¨¡å‹
        num_features = len(feature_names)
        model = self.create_model(num_features)

        # è®¡ç®—ç±»åˆ«æƒé‡
        train_labels = []
        for _, target in train_loader:
            train_labels.extend(target.numpy())
        class_weights = self.compute_class_weights(np.array(train_labels))

        # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='max', factor=0.5, patience=patience//3
        )
        criterion = nn.CrossEntropyLoss()

        # è®­ç»ƒå¾ªç¯
        best_val_auc = 0.0
        patience_counter = 0
        best_model_state = None

        for epoch in range(epochs):
            logger.info(f"\n{'='*50}")
            logger.info(f"ğŸ“… Epoch {epoch+1}/{epochs}")
            logger.info(f"{'='*50}")

            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(
                model, train_loader, optimizer, criterion, class_weights
            )

            # éªŒè¯
            val_loss, val_acc, val_metrics = self.evaluate(model, val_loader, criterion)
            val_auc = val_metrics['auc']

            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step(val_auc)

            # è®°å½•å†å²
            epoch_record = {
                'epoch': epoch + 1,
                'train_loss': train_loss,
                'train_acc': train_acc,
                'val_loss': val_loss,
                'val_acc': val_acc,
                'val_auc': val_auc,
                'lr': optimizer.param_groups[0]['lr']
            }
            self.history.append(epoch_record)

            # æ—¥å¿—
            logger.info(f"ğŸ“Š è®­ç»ƒ - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%")
            logger.info(f"ğŸ“Š éªŒè¯ - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%, AUC: {val_auc:.4f}")
            logger.info(f"ğŸ¯ å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.6f}")

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_auc > best_val_auc:
                best_val_auc = val_auc
                patience_counter = 0
                best_model_state = model.state_dict().copy()
                logger.info(f"ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹! AUC: {val_auc:.4f}")
            else:
                patience_counter += 1
                logger.info(f"â³ ç­‰å¾…æ”¹è¿›: {patience_counter}/{patience}")

            # æ—©åœ
            if patience_counter >= patience:
                logger.info(f"â¹ï¸ æ—©åœè§¦å‘! æœ€ä½³AUC: {best_val_auc:.4f}")
                break

        # åŠ è½½æœ€ä½³æ¨¡å‹
        if best_model_state is not None:
            model.load_state_dict(best_model_state)

        # æœ€ç»ˆè¯„ä¼°
        logger.info("\n" + "="*50)
        logger.info("ğŸ§ª æœ€ç»ˆæµ‹è¯•è¯„ä¼°")
        logger.info("="*50)

        test_loss, test_acc, test_metrics = self.evaluate(model, test_loader, criterion)

        logger.info(f"ğŸ“Š æµ‹è¯•æŸå¤±: {test_loss:.4f}")
        logger.info(f"ğŸ“Š æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2f}%")
        logger.info(f"ğŸ“Š æµ‹è¯•AUC: {test_metrics['auc']:.4f}")

        # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
        report = test_metrics['classification_report']
        logger.info(f"ğŸ¯ Phishing F1: {report['phish']['f1-score']:.4f}")
        logger.info(f"ğŸ¯ Benign F1: {report['benign']['f1-score']:.4f}")

        # ä¿å­˜æ¨¡å‹
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_name = f"real_phishing_advanced_{timestamp}"

        model_path = self.model_dir / f"{model_name}.pt"
        torch.save({
            'model_state_dict': model.state_dict(),
            'scaler_mean': scaler.mean_,
            'scaler_scale': scaler.scale_,
            'feature_names': feature_names,
            'model_config': {
                'num_features': num_features,
                'model_type': 'real_phishing_advanced'
            },
            'class_weights': class_weights.cpu().numpy(),
            'history': self.history,
            'metrics': {
                'test_acc': test_acc,
                'test_auc': test_metrics['auc'],
                'test_report': report,
                'best_epoch': epoch + 1 - patience_counter
            }
        }, model_path)

        logger.info(f"ğŸ’¾ çœŸå®é’“é±¼ç½‘ç«™æ¨¡å‹å·²ä¿å­˜: {model_path}")

        # ä¿å­˜è®­ç»ƒå†å²
        history_path = self.model_dir / f"training_history_{model_name}.json"
        with open(history_path, 'w') as f:
            json.dump(self.history, f, indent=2, ensure_ascii=False)

        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        self.plot_training_history(model_name)

        logger.info(f"ğŸ‰ çœŸå®é’“é±¼ç½‘ç«™æ¨¡å‹è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯AUC: {best_val_auc:.4f}")

        return TrainingArtifacts(
            model_state=model.state_dict(),
            scaler_mean=scaler.mean_,
            scaler_scale=scaler.scale_,
            feature_names=tuple(feature_names),
            class_weights=class_weights.cpu().numpy(),
            metrics={
                'test_acc': test_acc,
                'test_auc': test_metrics['auc'],
                'test_report': report
            },
            history=self.history
        )

    def plot_training_history(self, model_name: str):
        """ç»˜åˆ¶è®­ç»ƒå†å²"""
        try:
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

            epochs = [h['epoch'] for h in self.history]

            # æŸå¤±æ›²çº¿
            ax1.plot(epochs, [h['train_loss'] for h in self.history], 'b-', label='è®­ç»ƒæŸå¤±')
            ax1.plot(epochs, [h['val_loss'] for h in self.history], 'r-', label='éªŒè¯æŸå¤±')
            ax1.set_title('Loss Curve')
            ax1.set_xlabel('Epoch')
            ax1.set_ylabel('Loss')
            ax1.legend()
            ax1.grid(True)

            # å‡†ç¡®ç‡æ›²çº¿
            ax2.plot(epochs, [h['train_acc'] for h in self.history], 'b-', label='è®­ç»ƒå‡†ç¡®ç‡')
            ax2.plot(epochs, [h['val_acc'] for h in self.history], 'r-', label='éªŒè¯å‡†ç¡®ç‡')
            ax2.set_title('Accuracy Curve')
            ax2.set_xlabel('Epoch')
            ax2.set_ylabel('Accuracy (%)')
            ax2.legend()
            ax2.grid(True)

            # AUCæ›²çº¿
            ax3.plot(epochs, [h['val_auc'] for h in self.history], 'g-', label='éªŒè¯AUC')
            ax3.set_title('AUC Curve')
            ax3.set_xlabel('Epoch')
            ax3.set_ylabel('AUC')
            ax3.legend()
            ax3.grid(True)

            # å­¦ä¹ ç‡æ›²çº¿
            ax4.plot(epochs, [h['lr'] for h in self.history], 'm-', label='å­¦ä¹ ç‡')
            ax4.set_title('Learning Rate')
            ax4.set_xlabel('Epoch')
            ax4.set_ylabel('Learning Rate')
            ax4.set_yscale('log')
            ax4.legend()
            ax4.grid(True)

            plt.tight_layout()
            plot_path = self.model_dir / f"training_curves_{model_name}.png"
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            plt.close()

            logger.info(f"ğŸ“ˆ è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {plot_path}")

        except Exception as e:
            logger.warning(f"ç»˜åˆ¶è®­ç»ƒæ›²çº¿å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="è®­ç»ƒçœŸå®é’“é±¼ç½‘ç«™æ£€æµ‹æ¨¡å‹")
    parser.add_argument("--data-dir", type=str, default="data_real_phishing_v2",
                       help="æ•°æ®é›†ç›®å½•")
    parser.add_argument("--epochs", type=int, default=100,
                       help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--lr", type=float, default=0.001,
                       help="å­¦ä¹ ç‡")
    parser.add_argument("--patience", type=int, default=15,
                       help="æ—©åœè€å¿ƒå€¼")
    parser.add_argument("--device", type=str, default="auto",
                       choices=["auto", "cuda", "cpu"],
                       help="è®¡ç®—è®¾å¤‡")

    args = parser.parse_args()

    # è®¾ç½®æ—¥å¿—
    logger.add("logs/real_phishing_training_{time}.log", rotation="10 MB", level="INFO")

    # å¼€å§‹è®­ç»ƒ
    trainer = RealPhishingTrainer(
        data_dir=args.data_dir,
        device=args.device
    )

    start_time = time.time()
    artifacts = trainer.train(
        epochs=args.epochs,
        lr=args.lr,
        patience=args.patience
    )
    end_time = time.time()

    training_time = end_time - start_time
    logger.info(f"\nğŸ‰ çœŸå®é’“é±¼ç½‘ç«™æ¨¡å‹è®­ç»ƒå®Œæˆ!")
    logger.info(f"â±ï¸ æ€»è®­ç»ƒæ—¶é—´: {training_time/60:.1f} åˆ†é’Ÿ")
    logger.info(f"ğŸ“Š æœ€ç»ˆæµ‹è¯•AUC: {artifacts.metrics['test_auc']:.4f}")
    logger.info(f"ğŸ“Š æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {artifacts.metrics['test_acc']:.2f}%")

if __name__ == "__main__":
    main()