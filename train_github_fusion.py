#!/usr/bin/env python3
"""
ä½¿ç”¨GitHubæ•°æ®å’Œç°æœ‰æ•°æ®è®­ç»ƒç®€åŒ–ç‰ˆèåˆæ¨¡å‹
"""

import pandas as pd
import numpy as np
from pathlib import Path
import torch
import torch.nn as nn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import json
import time
import sys
sys.path.append('.')

from phishguard_v1.models.fusion_model import FusionDNN

# å…±åŒç‰¹å¾åˆ—è¡¨ï¼ˆä¸¤ä¸ªæ•°æ®é›†éƒ½æœ‰çš„ç‰¹å¾ï¼‰
COMMON_FEATURES = [
    'url_len', 'host_len', 'path_len', 'num_digits', 'num_letters', 'num_specials',
    'num_dots', 'num_hyphen', 'num_slash', 'num_qm', 'num_at', 'num_pct',
    'has_ip', 'subdomain_depth', 'tld_suspicious', 'has_punycode', 'scheme_https',
    'query_len', 'fragment_len', 'status_code', 'bytes'
]

def load_and_merge_data():
    """åŠ è½½å¹¶åˆå¹¶æ•°æ®"""
    print("ğŸ“¦ åŠ è½½æ•°æ®é›†...")

    # åŠ è½½ç°æœ‰æ•°æ®
    try:
        df_old = pd.read_parquet('data/dataset.parquet')
        print(f"  ç°æœ‰æ•°æ®: {len(df_old)} æ¡è®°å½•")
    except Exception as e:
        print(f"âŒ æ— æ³•åŠ è½½ç°æœ‰æ•°æ®: {e}")
        return None

    # åŠ è½½GitHubæ•°æ®
    try:
        df_github = pd.read_parquet('validated_github_data/training_data.parquet')
        print(f"  GitHubæ•°æ®: {len(df_github)} æ¡è®°å½•")
    except Exception as e:
        print(f"âŒ æ— æ³•åŠ è½½GitHubæ•°æ®: {e}")
        return None

    # æå–å…±åŒç‰¹å¾
    old_features = df_old[COMMON_FEATURES + ['label', 'url']].copy()
    github_features = df_github[COMMON_FEATURES + ['label', 'url']].copy()

    # åˆå¹¶æ•°æ®
    merged_df = pd.concat([old_features, github_features], ignore_index=True)

    # å»é‡
    initial_count = len(merged_df)
    merged_df = merged_df.drop_duplicates(subset=['url'], keep='first')
    final_count = len(merged_df)

    print(f"  åˆå¹¶å: {final_count} æ¡è®°å½•")
    print(f"  å»é‡å‡å°‘: {initial_count - final_count} æ¡")

    # ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ
    phishing_count = len(merged_df[merged_df['label'] == 1])
    benign_count = len(merged_df[merged_df['label'] == 0])
    print(f"  é’“é±¼ç½‘ç«™: {phishing_count} ({phishing_count/final_count*100:.1f}%)")
    print(f"  è‰¯æ€§ç½‘ç«™: {benign_count} ({benign_count/final_count*100:.1f}%)")

    return merged_df

def prepare_data(df):
    """å‡†å¤‡è®­ç»ƒæ•°æ®"""
    print("ğŸ”§ å‡†å¤‡è®­ç»ƒæ•°æ®...")

    # å¤åˆ¶æ•°æ®æ¡†é¿å…ä¿®æ”¹åŸæ•°æ®
    df_processed = df.copy()

    # è½¬æ¢booleanç±»å‹ä¸ºint
    for col in COMMON_FEATURES:
        if df_processed[col].dtype == 'bool':
            df_processed[col] = df_processed[col].astype(int)

    # æå–ç‰¹å¾å’Œæ ‡ç­¾
    X = df_processed[COMMON_FEATURES].values.astype(float)
    y = df['label'].values

    # å¤„ç†ç¼ºå¤±å€¼
    mask = ~np.isnan(X).any(axis=1)
    X = X[mask]
    y = y[mask]

    print(f"  æœ‰æ•ˆæ ·æœ¬: {len(X)}")
    print(f"  ç‰¹å¾ç»´åº¦: {X.shape[1]}")

    # æ•°æ®åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    return X_train_scaled, X_test_scaled, y_train, y_test, scaler

def train_model(X_train, X_test, y_train, y_test):
    """è®­ç»ƒæ¨¡å‹"""
    print("ğŸ§  è®­ç»ƒèåˆæ¨¡å‹...")

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"  ä½¿ç”¨è®¾å¤‡: {device}")

    # åˆ›å»ºæ¨¡å‹
    input_dim = X_train.shape[1]
    model = FusionDNN(num_features=input_dim).to(device)

    # è½¬æ¢ä¸ºå¼ é‡
    X_train_tensor = torch.FloatTensor(X_train).to(device)
    y_train_tensor = torch.FloatTensor(y_train).to(device)
    X_test_tensor = torch.FloatTensor(X_test).to(device)
    y_test_tensor = torch.FloatTensor(y_test).to(device)

    # è®­ç»ƒå‚æ•°
    num_epochs = 150
    batch_size = 32
    learning_rate = 0.001

    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    criterion = nn.CrossEntropyLoss().to(device)  # ä½¿ç”¨äº¤å‰ç†µæŸå¤±

    # è®­ç»ƒå¾ªç¯
    model.train()
    for epoch in range(num_epochs):
        total_loss = 0
        num_batches = 0

        for i in range(0, len(X_train_tensor), batch_size):
            batch_X = X_train_tensor[i:i + batch_size]
            batch_y = y_train_tensor[i:i + batch_size].long()  # CrossEntropyéœ€è¦longç±»å‹

            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            num_batches += 1

        avg_loss = total_loss / num_batches

        if (epoch + 1) % 30 == 0:
            print(f"    è½®æ¬¡ {epoch+1}/{num_epochs}, å¹³å‡æŸå¤±: {avg_loss:.4f}")

    # è¯„ä¼°
    model.eval()
    with torch.no_grad():
        test_outputs = model(X_test_tensor)
        test_predictions = torch.argmax(test_outputs, dim=1).cpu().numpy()

        accuracy = accuracy_score(y_test, test_predictions)
        precision = precision_score(y_test, test_predictions, zero_division=0)
        recall = recall_score(y_test, test_predictions, zero_division=0)
        f1 = f1_score(y_test, test_predictions, zero_division=0)

        print(f"  æµ‹è¯•æ€§èƒ½:")
        print(f"    å‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"    ç²¾ç¡®ç‡: {precision:.4f}")
        print(f"    å¬å›ç‡: {recall:.4f}")
        print(f"    F1åˆ†æ•°: {f1:.4f}")

    return model, {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1
    }

def save_model(model, scaler, metrics):
    """ä¿å­˜æ¨¡å‹"""
    print("ğŸ’¾ ä¿å­˜æ¨¡å‹...")

    save_dir = Path("artifacts")
    save_dir.mkdir(exist_ok=True)

    checkpoint = {
        'model_state_dict': model.state_dict(),
        'input_features': len(COMMON_FEATURES),
        'feature_names': COMMON_FEATURES,
        'scaler_mean': scaler.mean_.tolist(),
        'scaler_scale': scaler.scale_.tolist(),
        'metrics': metrics,
        'training_time': time.time(),
        'model_type': 'github_enhanced_fusion'
    }

    model_path = save_dir / "fusion_github_simple.pt"
    torch.save(checkpoint, model_path)
    print(f"  æ¨¡å‹å·²ä¿å­˜: {model_path}")

    return model_path

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ è®­ç»ƒGitHubå¢å¼ºèåˆæ¨¡å‹")
    print("=" * 50)

    # 1. åŠ è½½å’Œåˆå¹¶æ•°æ®
    df = load_and_merge_data()
    if df is None:
        return

    # 2. å‡†å¤‡æ•°æ®
    X_train, X_test, y_train, y_test, scaler = prepare_data(df)

    # 3. è®­ç»ƒæ¨¡å‹
    model, metrics = train_model(X_train, X_test, y_train, y_test)

    # 4. ä¿å­˜æ¨¡å‹
    model_path = save_model(model, scaler, metrics)

    # 5. è¾“å‡ºæ€»ç»“
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ“Š æ¨¡å‹ä¿¡æ¯:")
    print(f"  è®­ç»ƒæ ·æœ¬: {len(df)}")
    print(f"  ç‰¹å¾ç»´åº¦: {len(COMMON_FEATURES)}")
    print(f"  æ¨¡å‹æ€§èƒ½: å‡†ç¡®ç‡ {metrics['accuracy']:.2%}, å¬å›ç‡ {metrics['recall']:.2%}")

    print(f"\nğŸ”§ ä½¿ç”¨æ–¹æ³•:")
    print(f"  1. æ›´æ–°APIé…ç½®ä½¿ç”¨æ¨¡å‹: {model_path}")
    print(f"  2. æµ‹è¯•æ–°æ¨¡å‹æ€§èƒ½")
    print(f"  3. å¯¹æ¯”ä¸åŸæ¨¡å‹çš„æ”¹è¿›")

if __name__ == "__main__":
    main()