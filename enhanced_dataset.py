#!/usr/bin/env python3
"""
å¢å¼ºæ•°æ®é›†æ„å»ºè„šæœ¬
æ”¶é›†æ›´å¤šé«˜è´¨é‡ã€å¤šæ ·åŒ–çš„è®­ç»ƒæ•°æ®
"""

import asyncio
import pandas as pd
from typing import List, Dict, Any
from phishguard_v1.features.fetcher import fetch_one
from phishguard_v1.features.parser import extract_from_html
from phishguard_v1.features.url_features import url_stats
from httpx import AsyncClient
import numpy as np
from pathlib import Path

# é«˜è´¨é‡è‰¯æ€§ç½‘ç«™æ¥æº
BENIGN_SOURCES = {
    "çŸ¥åç§‘æŠ€å…¬å¸": [
        "https://www.google.com",
        "https://www.microsoft.com",
        "https://www.apple.com",
        "https://www.amazon.com",
        "https://www.facebook.com",
        "https://www.twitter.com",
        "https://www.linkedin.com",
        "https://www.github.com",
        "https://www.stackoverflow.com",
        "https://www.wikipedia.org",
        "https://www.youtube.com",
        "https://www.instagram.com",
        "https://www.netflix.com",
        "https://www.spotify.com",
        "https://www.airbnb.com",
    ],
    "ä¸­å›½çŸ¥åç½‘ç«™": [
        "https://www.baidu.com",
        "https://www.alibaba.com",
        "https://www.tmall.com",
        "https://www.qq.com",
        "https://www.weibo.com",
        "https://www.zhihu.com",
        "https://www.douban.com",
        "https://www.taobao.com",
        "https://www.jd.com",
        "https://www.meituan.com",
        "https://www.didi.com",
        "https://www.bytedance.com",
        "https://www.xiaohongshu.com",
    ],
    "æ”¿åºœæœºæ„": [
        "https://www.gov.cn",
        "https://www.whitehouse.gov",
        "https://www.europa.eu",
        "https://www.parliament.uk",
        "https://www.canada.ca",
    ],
    "é‡‘èæœºæ„": [
        "https://www.bankofamerica.com",
        "https://www.chase.com",
        "https://www.wellsfargo.com",
        "https://www.citibank.com",
        "https://www.hsbc.com",
        "https://www.icbc.com.cn",
        "https://www.bankofchina.com",
    ]
}

# é’“é±¼ç½‘ç«™æ¥æºï¼ˆä»å…¬å¼€æ•°æ®æºè·å–ï¼‰
PHISH_SOURCES = [
    # å¯ä»¥ä»è¿™äº›æ¥æºè·å–æ›´å¤šé’“é±¼ç½‘ç«™æ•°æ®
    "https://openphish.com/",
    "https://phishtank.org/",
    "https://urlhaus.abuse.ch/browse/",
]

async def collect_benign_data(urls: List[str], max_concurrent: int = 10) -> List[Dict[str, Any]]:
    """æ”¶é›†è‰¯æ€§ç½‘ç«™æ•°æ®"""
    results = []

    async with AsyncClient(
        timeout=30.0,
        headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
    ) as client:

        # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…è¿‡å¤šå¹¶å‘
        for i in range(0, len(urls), max_concurrent):
            batch = urls[i:i + max_concurrent]
            tasks = [fetch_single_url(url, client) for url in batch]
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)

            for result in batch_results:
                if isinstance(result, dict) and result.get("ok"):
                    results.append(result)
                elif isinstance(result, Exception):
                    print(f"æŠ“å–å¤±è´¥: {result}")

            print(f"å·²å®Œæˆ {min(i + max_concurrent, len(urls))}/{len(urls)} ä¸ªè‰¯æ€§ç½‘ç«™")
            await asyncio.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«

    return results

async def fetch_single_url(url: str, client: AsyncClient) -> Dict[str, Any]:
    """æŠ“å–å•ä¸ªURLå¹¶æå–ç‰¹å¾"""
    try:
        # æŠ“å–ç½‘ç«™
        item = await fetch_one(url, client)

        if not item or not item.get("ok"):
            return {"url": url, "error": "æŠ“å–å¤±è´¥", "ok": False}

        # æå–HTMLç‰¹å¾
        html_content = item.get("html", "")
        final_url = item.get("final_url") or url
        item["html_feats"] = extract_from_html(html_content, final_url)

        # ç¡®ä¿æœ‰URLç‰¹å¾
        if "url_feats" not in item or not item["url_feats"]:
            item["url_feats"] = url_stats(final_url)

        # æ·»åŠ æ ‡ç­¾ (0 = è‰¯æ€§)
        item["label"] = 0
        item["source"] = "benign"
        item["category"] = "trusted"

        return item

    except Exception as e:
        return {"url": url, "error": str(e), "ok": False}

def load_existing_data() -> pd.DataFrame:
    """åŠ è½½ç°æœ‰æ•°æ®"""
    data_path = Path("data")
    if not data_path.exists():
        return pd.DataFrame()

    dfs = []
    for file in ["train.parquet", "test.parquet", "val.parquet"]:
        file_path = data_path / file
        if file_path.exists():
            df = pd.read_parquet(file_path)
            dfs.append(df)

    if dfs:
        return pd.concat(dfs, ignore_index=True)
    return pd.DataFrame()

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ”¶é›†å¢å¼ºè®­ç»ƒæ•°æ®...")

    # åŠ è½½ç°æœ‰æ•°æ®
    existing_df = load_existing_data()
    print(f"ç°æœ‰æ•°æ®: {len(existing_df)} æ¡è®°å½•")

    # æ”¶é›†è‰¯æ€§æ•°æ®
    all_benign_urls = []
    for category, urls in BENIGN_SOURCES.items():
        print(f"å‡†å¤‡æ”¶é›† {category} ç±»åˆ«çš„ç½‘ç«™...")
        all_benign_urls.extend(urls)

    print(f"æ€»å…±éœ€è¦æ”¶é›† {len(all_benign_urls)} ä¸ªè‰¯æ€§ç½‘ç«™")

    # æ”¶é›†æ–°æ•°æ®
    benign_data = await collect_benign_data(all_benign_urls)
    print(f"æˆåŠŸæ”¶é›† {len(benign_data)} ä¸ªè‰¯æ€§ç½‘ç«™")

    # è½¬æ¢ä¸ºDataFrame
    new_data = []
    for item in benign_data:
        if item.get("ok"):
            row = {
                "request_url": item.get("request_url"),
                "final_url": item.get("final_url"),
                "status_code": item.get("status_code"),
                "content_type": item.get("content_type"),
                "bytes": item.get("bytes"),
                "html": item.get("html", "")[:1000],  # åªä¿å­˜éƒ¨åˆ†HTMLä»¥èŠ‚çœç©ºé—´
                "label": item.get("label", 0),
                "source": item.get("source", "unknown"),
                "category": item.get("category", "unknown"),
            }

            # æ·»åŠ URLç‰¹å¾
            url_feats = item.get("url_feats", {})
            for k, v in url_feats.items():
                row[k] = v

            # æ·»åŠ HTMLç‰¹å¾
            html_feats = item.get("html_feats", {})
            for k, v in html_feats.items():
                row[k] = v

            new_data.append(row)

    new_df = pd.DataFrame(new_data)
    print(f"æ–°æ•°æ®è½¬æ¢å®Œæˆ: {len(new_df)} æ¡æœ‰æ•ˆè®°å½•")

    # åˆå¹¶æ•°æ®
    if not existing_df.empty:
        # å»é‡ - åŸºäºfinal_url
        combined_df = pd.concat([existing_df, new_df], ignore_index=True)
        combined_df = combined_df.drop_duplicates(subset=["final_url"], keep="last")
    else:
        combined_df = new_df

    print(f"åˆå¹¶åæ•°æ®: {len(combined_df)} æ¡è®°å½•")

    # æ•°æ®ç»Ÿè®¡
    benign_count = len(combined_df[combined_df["label"] == 0])
    phishing_count = len(combined_df[combined_df["label"] == 1])

    print(f"è‰¯æ€§ç½‘ç«™: {benign_count} ({benign_count/len(combined_df)*100:.1f}%)")
    print(f"é’“é±¼ç½‘ç«™: {phishing_count} ({phishing_count/len(combined_df)*100:.1f}%)")

    # ä¿å­˜æ•°æ®
    output_dir = Path("data_enhanced")
    output_dir.mkdir(exist_ok=True)

    # åˆ†å‰²æ•°æ®é›†
    from sklearn.model_selection import train_test_split

    train_df, temp_df = train_test_split(combined_df, test_size=0.3, random_state=42, stratify=combined_df["label"])
    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df["label"])

    train_df.to_parquet(output_dir / "train.parquet")
    val_df.to_parquet(output_dir / "val.parquet")
    test_df.to_parquet(output_dir / "test.parquet")

    print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ° {output_dir}/")
    print(f"è®­ç»ƒé›†: {len(train_df)} æ¡")
    print(f"éªŒè¯é›†: {len(val_df)} æ¡")
    print(f"æµ‹è¯•é›†: {len(test_df)} æ¡")

    # ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š
    generate_data_report(combined_df, output_dir)

def generate_data_report(df: pd.DataFrame, output_dir: Path):
    """ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š"""
    report = f"""
# å¢å¼ºæ•°æ®é›†è´¨é‡æŠ¥å‘Š

## æ•°æ®ç»Ÿè®¡
- æ€»æ ·æœ¬æ•°: {len(df)}
- ç‰¹å¾æ•°é‡: {len([col for col in df.columns if col not in ['request_url', 'final_url', 'html', 'source', 'category', 'label']])}
- æ•°æ®å¤§å°: {df.memory_usage(deep=True).sum() / 1024 / 1024:.1f} MB

## æ ‡ç­¾åˆ†å¸ƒ
- è‰¯æ€§ç½‘ç«™: {len(df[df['label'] == 0])} ({len(df[df['label'] == 0])/len(df)*100:.1f}%)
- é’“é±¼ç½‘ç«™: {len(df[df['label'] == 1])} ({len(df[df['label'] == 1])/len(df)*100:.1f}%)

## æ¥æºåˆ†å¸ƒ
"""

    if 'source' in df.columns:
        source_counts = df['source'].value_counts()
        for source, count in source_counts.items():
            report += f"- {source}: {count} ({count/len(df)*100:.1f}%)\n"

    # ä¿å­˜æŠ¥å‘Š
    with open(output_dir / "data_report.md", "w", encoding="utf-8") as f:
        f.write(report)

    print(f"ğŸ“Š æ•°æ®è´¨é‡æŠ¥å‘Šå·²ä¿å­˜: {output_dir}/data_report.md")

if __name__ == "__main__":
    asyncio.run(main())