#!/usr/bin/env python3
"""
é‡å»ºå¢å¼ºæ•°æ®é›†
ä½¿ç”¨ç°æœ‰æ•°æ®ä½†æ·»åŠ æ–°çš„å¢å¼ºç‰¹å¾
"""

import pandas as pd
import asyncio
from pathlib import Path
from phishguard_v1.features.parser import extract_from_html
from phishguard_v1.features.url_features import url_stats
from httpx import AsyncClient
from typing import Dict, Any
import numpy as np

def reprocess_existing_data():
    """é‡æ–°å¤„ç†ç°æœ‰æ•°æ®ï¼Œæ·»åŠ å¢å¼ºç‰¹å¾"""
    print("ğŸ”„ é‡æ–°å¤„ç†ç°æœ‰æ•°æ®...")

    # åŠ è½½ç°æœ‰æ•°æ®
    data_path = Path("data")
    if not data_path.exists():
        print("âŒ ç°æœ‰æ•°æ®ç›®å½•ä¸å­˜åœ¨")
        return

    dfs = []
    for file in ["train.parquet", "val.parquet", "test.parquet"]:
        file_path = data_path / file
        if file_path.exists():
            df = pd.read_parquet(file_path)
            dfs.append(df)

    if not dfs:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ç°æœ‰æ•°æ®")
        return

    combined_df = pd.concat(dfs, ignore_index=True)
    print(f"ğŸ“Š åŠ è½½äº† {len(combined_df)} æ¡ç°æœ‰è®°å½•")

    # é‡æ–°å¤„ç†æ¯æ¡è®°å½•
    enhanced_records = []

    for idx, row in combined_df.iterrows():
        if idx % 100 == 0:
            print(f"å¤„ç†ä¸­: {idx}/{len(combined_df)}")

        # åŸºç¡€æ•°æ®
        record = {
            "request_url": row.get("request_url", ""),
            "final_url": row.get("final_url", row.get("request_url", "")),
            "status_code": row.get("status_code", 200),
            "content_type": row.get("content_type", ""),
            "bytes": row.get("bytes", 0),
            "html": row.get("html", ""),
            "label": row.get("label", 0),
            "source": row.get("source", "existing"),
            "category": row.get("category", "unknown"),
        }

        # æ·»åŠ ç°æœ‰çš„URLç‰¹å¾
        for col in ["url_len", "host_len", "path_len", "num_digits", "num_letters", "num_specials",
                   "num_dots", "num_hyphen", "num_slash", "num_qm", "num_at", "num_pct",
                   "has_ip", "subdomain_depth", "tld_suspicious", "has_punycode", "scheme_https",
                   "query_len", "fragment_len"]:
            record[col] = row.get(col, 0)

        # æ·»åŠ ç°æœ‰çš„HTMLç‰¹å¾
        for col in ["has_html", "title_len", "num_meta", "num_links", "num_stylesheets",
                   "num_scripts", "num_script_ext", "num_script_inline", "num_iframes",
                   "num_forms", "has_password_input", "has_email_input", "suspicious_js_inline"]:
            record[col] = row.get(col, 0)

        # å¦‚æœæœ‰HTMLå†…å®¹ï¼Œé‡æ–°æå–å¢å¼ºç‰¹å¾
        html_content = row.get("html", "")
        final_url = record["final_url"]

        if html_content and final_url:
            try:
                # ä½¿ç”¨å¢å¼ºçš„è§£æå™¨é‡æ–°æå–ç‰¹å¾
                html_feats = extract_from_html(html_content, final_url)

                # æ·»åŠ å¢å¼ºç‰¹å¾
                record["external_form_actions"] = html_feats.get("external_form_actions", 0)
                record["num_hidden_inputs"] = html_feats.get("num_hidden_inputs", 0)
                record["external_links"] = html_feats.get("external_links", 0)
                record["internal_links"] = html_feats.get("internal_links", 0)
                record["external_images"] = html_feats.get("external_images", 0)
                record["is_subdomain"] = html_feats.get("is_subdomain", 0)
                record["has_www"] = html_feats.get("has_www", 0)
                record["is_common_tld"] = html_feats.get("is_common_tld", 0)

            except Exception as e:
                print(f"å¤„ç†è®°å½• {idx} æ—¶å‡ºé”™: {e}")
                # ä½¿ç”¨é»˜è®¤å€¼
                record["external_form_actions"] = 0
                record["num_hidden_inputs"] = 0
                record["external_links"] = 0
                record["internal_links"] = 0
                record["external_images"] = 0
                record["is_subdomain"] = 0
                record["has_www"] = 0
                record["is_common_tld"] = 0
        else:
            # æ²¡æœ‰HTMLå†…å®¹ï¼Œä½¿ç”¨é»˜è®¤å€¼
            record["external_form_actions"] = 0
            record["num_hidden_inputs"] = 0
            record["external_links"] = 0
            record["internal_links"] = 0
            record["external_images"] = 0
            record["is_subdomain"] = 0
            record["has_www"] = 0
            record["is_common_tld"] = 0

        enhanced_records.append(record)

    # åˆ›å»ºæ–°çš„DataFrame
    enhanced_df = pd.DataFrame(enhanced_records)
    print(f"âœ… é‡æ–°å¤„ç†å®Œæˆï¼Œå…± {len(enhanced_df)} æ¡è®°å½•")

    # æ•°æ®ç»Ÿè®¡
    benign_count = len(enhanced_df[enhanced_df["label"] == 0])
    phishing_count = len(enhanced_df[enhanced_df["label"] == 1])

    print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"  è‰¯æ€§ç½‘ç«™: {benign_count} ({benign_count/len(enhanced_df)*100:.1f}%)")
    print(f"  é’“é±¼ç½‘ç«™: {phishing_count} ({phishing_count/len(enhanced_df)*100:.1f}%)")

    # ä¿å­˜æ•°æ®
    output_dir = Path("data_enhanced")
    output_dir.mkdir(exist_ok=True)

    # åˆ†å‰²æ•°æ®é›†
    from sklearn.model_selection import train_test_split

    train_df, temp_df = train_test_split(enhanced_df, test_size=0.3, random_state=42, stratify=enhanced_df["label"])
    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df["label"])

    train_df.to_parquet(output_dir / "train.parquet")
    val_df.to_parquet(output_dir / "val.parquet")
    test_df.to_parquet(output_dir / "test.parquet")

    print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ° {output_dir}/")
    print(f"è®­ç»ƒé›†: {len(train_df)} æ¡")
    print(f"éªŒè¯é›†: {len(val_df)} æ¡")
    print(f"æµ‹è¯•é›†: {len(test_df)} æ¡")

    return enhanced_df

if __name__ == "__main__":
    reprocess_existing_data()