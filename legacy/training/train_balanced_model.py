#!/usr/bin/env python3
"""
è®­ç»ƒå¹³è¡¡çš„èåˆæ¨¡å‹ï¼ŒåŒ…å«æ›´å¤šæ ·åŒ–çš„è‰¯æ€§ç½‘ç«™
"""

import pandas as pd
import numpy as np
from pathlib import Path
import torch
import torch.nn as nn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import json
import time
import sys
sys.path.append('.')

from phishguard_v1.models.fusion_model import FusionDNN
from phishguard_v1.models.dataset import NUMERIC_COLS

def create_balanced_training_data():
    """åˆ›å»ºå¹³è¡¡çš„è®­ç»ƒæ•°æ®"""
    print("ğŸ“¦ åˆ›å»ºå¹³è¡¡è®­ç»ƒæ•°æ®é›†...")

    # åŠ è½½ç°æœ‰æ•°æ®
    try:
        df_old = pd.read_parquet('data/dataset.parquet')
        print(f"  ç°æœ‰æ•°æ®: {len(df_old)} æ¡è®°å½•")
    except Exception as e:
        print(f"âŒ æ— æ³•åŠ è½½ç°æœ‰æ•°æ®: {e}")
        return None

    # åŠ è½½GitHubæ•°æ®
    try:
        df_github = pd.read_parquet('validated_github_data/training_data.parquet')
        print(f"  GitHubæ•°æ®: {len(df_github)} æ¡è®°å½•")
    except Exception as e:
        print(f"âŒ æ— æ³•åŠ è½½GitHubæ•°æ®: {e}")
        return None

    # æ·»åŠ æ›´å¤šè‰¯æ€§ç½‘ç«™æ ·ä¾‹
    additional_benign = [
        # ä¸­å›½çŸ¥åç½‘ç«™
        {'url': 'https://www.baidu.com', 'label': 0},
        {'url': 'https://www.taobao.com', 'label': 0},
        {'url': 'https://www.qq.com', 'label': 0},
        {'url': 'https://www.weibo.com', 'label': 0},
        {'url': 'https://www.zhihu.com', 'label': 0},
        {'url': 'https://www.douban.com', 'label': 0},
        {'url': 'https://www.tmall.com', 'label': 0},
        {'url': 'https://www.jd.com', 'label': 0},
        {'url': 'https://www.163.com', 'label': 0},
        {'url': 'https://www.sohu.com', 'label': 0},
        # å›½é™…çŸ¥åç½‘ç«™
        {'url': 'https://www.instagram.com', 'label': 0},
        {'url': 'https://www.linkedin.com', 'label': 0},
        {'url': 'https://www.github.com', 'label': 0},
        {'url': 'https://www.stackoverflow.com', 'label': 0},
        {'url': 'https://www.wikipedia.org', 'label': 0},
        # ä¸­ç­‰è§„æ¨¡è‰¯æ€§ç½‘ç«™
        {'url': 'https://medium.com', 'label': 0},
        {'url': 'https://www.reddit.com', 'label': 0},
        {'url': 'https://www.quora.com', 'label': 0},
        {'url': 'https://www.bilibili.com', 'label': 0},
        {'url': 'https://www.douyin.com', 'label': 0},
    ]

    # åˆ›å»ºé¢å¤–è‰¯æ€§ç½‘ç«™DataFrame
    df_additional = pd.DataFrame(additional_benign)
    print(f"  é¢å¤–è‰¯æ€§ç½‘ç«™: {len(df_additional)} æ¡è®°å½•")

    # åˆå¹¶æ‰€æœ‰æ•°æ®
    all_data = [df_old, df_github, df_additional]
    merged_df = pd.concat(all_data, ignore_index=True)

    # å»é‡
    initial_count = len(merged_df)
    merged_df = merged_df.drop_duplicates(subset=['url'], keep='first')
    final_count = len(merged_df)

    print(f"  åˆå¹¶å: {final_count} æ¡è®°å½•")
    print(f"  å»é‡å‡å°‘: {initial_count - final_count} æ¡")

    # ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ
    phishing_count = len(merged_df[merged_df['label'] == 1])
    benign_count = len(merged_df[merged_df['label'] == 0])
    print(f"  é’“é±¼ç½‘ç«™: {phishing_count} ({phishing_count/final_count*100:.1f}%)")
    print(f"  è‰¯æ€§ç½‘ç«™: {benign_count} ({benign_count/final_count*100:.1f}%)")

    return merged_df

def extract_features_for_url(url):
    """ä¸ºURLæå–ç‰¹å¾"""
    from urllib.parse import urlparse
    import re

    parsed = urlparse(url)
    hostname = parsed.hostname or ""
    path = parsed.path or ""
    query = parsed.query or ""
    fragment = parsed.fragment or ""

    # URLåŸºç¡€ç‰¹å¾
    features = {}

    features['url_len'] = len(url)
    features['host_len'] = len(hostname)
    features['path_len'] = len(path)
    features['query_len'] = len(query)
    features['fragment_len'] = len(fragment)

    # å­—ç¬¦ç»Ÿè®¡
    features['num_digits'] = len(re.findall(r'\d', url))
    features['num_letters'] = len(re.findall(r'[a-zA-Z]', url))
    features['num_specials'] = len(re.findall(r'[^\w\d]', url))
    features['num_dots'] = url.count('.')
    features['num_hyphen'] = url.count('-')
    features['num_slash'] = url.count('/')
    features['num_qm'] = url.count('?')
    features['num_at'] = url.count('@')
    features['num_pct'] = url.count('%')

    # åŸŸåç‰¹å¾
    features['has_ip'] = 1 if re.match(r'\d+\.\d+\.\d+\.\d+', hostname) else 0
    features['subdomain_depth'] = len(hostname.split('.')) - 2 if hostname.count('.') > 1 else 0
    features['tld_suspicious'] = 1 if hostname.split('.')[-1] in ['tk', 'ml', 'ga', 'cf', 'top'] else 0
    features['has_punycode'] = 1 if 'xn--' in hostname else 0
    features['scheme_https'] = 1 if parsed.scheme == 'https' else 0

    # HTTPå“åº”ç‰¹å¾ï¼ˆé»˜è®¤å€¼ï¼‰
    features['status_code'] = 200
    features['bytes'] = 1024  # å‡è®¾çš„å¹³å‡å¤§å°

    return features

def prepare_balanced_data(df):
    """å‡†å¤‡å¹³è¡¡çš„è®­ç»ƒæ•°æ®"""
    print("ğŸ”§ å‡†å¤‡å¹³è¡¡è®­ç»ƒæ•°æ®...")

    # ä¸ºæ¯ä¸ªURLæå–ç‰¹å¾
    feature_data = []
    for _, row in df.iterrows():
        url = row['url']
        label = row['label']
        features = extract_features_for_url(url)
        features['label'] = label
        feature_data.append(features)

    df_features = pd.DataFrame(feature_data)

    # ç¡®ä¿æ‰€æœ‰ç‰¹å¾åˆ—éƒ½å­˜åœ¨
    required_features = [
        'url_len', 'host_len', 'path_len', 'num_digits', 'num_letters', 'num_specials',
        'num_dots', 'num_hyphen', 'num_slash', 'num_qm', 'num_at', 'num_pct',
        'has_ip', 'subdomain_depth', 'tld_suspicious', 'has_punycode', 'scheme_https',
        'query_len', 'fragment_len', 'status_code', 'bytes'
    ]

    for feat in required_features:
        if feat not in df_features.columns:
            df_features[feat] = 0

    # è½¬æ¢booleanç±»å‹
    for col in ['has_ip', 'tld_suspicious', 'has_punycode', 'scheme_https']:
        df_features[col] = df_features[col].astype(int)

    # æå–ç‰¹å¾å’Œæ ‡ç­¾
    X = df_features[required_features].values.astype(float)
    y = df_features['label'].values

    # å¤„ç†ç¼ºå¤±å€¼
    mask = ~np.isnan(X).any(axis=1)
    X = X[mask]
    y = y[mask]

    print(f"  æœ‰æ•ˆæ ·æœ¬: {len(X)}")
    print(f"  ç‰¹å¾ç»´åº¦: {X.shape[1]}")

    # æ•°æ®åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    return X_train_scaled, X_test_scaled, y_train, y_test, scaler

def train_balanced_model(X_train, X_test, y_train, y_test):
    """è®­ç»ƒå¹³è¡¡æ¨¡å‹"""
    print("ğŸ§  è®­ç»ƒå¹³è¡¡èåˆæ¨¡å‹...")

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"  ä½¿ç”¨è®¾å¤‡: {device}")

    # åˆ›å»ºæ¨¡å‹
    input_dim = X_train.shape[1]
    model = FusionDNN(num_features=input_dim).to(device)

    # è½¬æ¢ä¸ºå¼ é‡
    X_train_tensor = torch.FloatTensor(X_train).to(device)
    y_train_tensor = torch.FloatTensor(y_train).to(device)
    X_test_tensor = torch.FloatTensor(X_test).to(device)
    y_test_tensor = torch.FloatTensor(y_test).to(device)

    # è®­ç»ƒå‚æ•°
    num_epochs = 200
    batch_size = 32
    learning_rate = 0.001

    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    criterion = nn.CrossEntropyLoss().to(device)

    # è®­ç»ƒå¾ªç¯
    model.train()
    for epoch in range(num_epochs):
        total_loss = 0
        num_batches = 0

        for i in range(0, len(X_train_tensor), batch_size):
            batch_X = X_train_tensor[i:i + batch_size]
            batch_y = y_train_tensor[i:i + batch_size].long()

            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            num_batches += 1

        avg_loss = total_loss / num_batches

        if (epoch + 1) % 40 == 0:
            print(f"    è½®æ¬¡ {epoch+1}/{num_epochs}, å¹³å‡æŸå¤±: {avg_loss:.4f}")

    # è¯„ä¼°
    model.eval()
    with torch.no_grad():
        test_outputs = model(X_test_tensor)
        test_predictions = torch.argmax(test_outputs, dim=1).cpu().numpy()

        accuracy = accuracy_score(y_test, test_predictions)
        precision = precision_score(y_test, test_predictions, zero_division=0)
        recall = recall_score(y_test, test_predictions, zero_division=0)
        f1 = f1_score(y_test, test_predictions, zero_division=0)

        print(f"  æµ‹è¯•æ€§èƒ½:")
        print(f"    å‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"    ç²¾ç¡®ç‡: {precision:.4f}")
        print(f"    å¬å›ç‡: {recall:.4f}")
        print(f"    F1åˆ†æ•°: {f1:.4f}")

    return model, {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1
    }

def save_balanced_model(model, scaler, metrics):
    """ä¿å­˜å¹³è¡¡æ¨¡å‹"""
    print("ğŸ’¾ ä¿å­˜å¹³è¡¡æ¨¡å‹...")

    save_dir = Path("artifacts")
    save_dir.mkdir(exist_ok=True)

    feature_names = [
        'url_len', 'host_len', 'path_len', 'num_digits', 'num_letters', 'num_specials',
        'num_dots', 'num_hyphen', 'num_slash', 'num_qm', 'num_at', 'num_pct',
        'has_ip', 'subdomain_depth', 'tld_suspicious', 'has_punycode', 'scheme_https',
        'query_len', 'fragment_len', 'status_code', 'bytes'
    ]

    checkpoint = {
        'model_state_dict': model.state_dict(),
        'input_features': len(feature_names),
        'feature_names': feature_names,
        'scaler_mean': scaler.mean_.tolist(),
        'scaler_scale': scaler.scale_.tolist(),
        'metrics': metrics,
        'training_time': time.time(),
        'model_type': 'balanced_fusion_v2'
    }

    model_path = save_dir / "fusion_balanced_v2.pt"
    torch.save(checkpoint, model_path)
    print(f"  æ¨¡å‹å·²ä¿å­˜: {model_path}")

    return model_path

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ è®­ç»ƒå¹³è¡¡èåˆæ¨¡å‹ v2")
    print("=" * 50)

    # 1. åˆ›å»ºå¹³è¡¡æ•°æ®
    df = create_balanced_training_data()
    if df is None:
        return

    # 2. å‡†å¤‡æ•°æ®
    X_train, X_test, y_train, y_test, scaler = prepare_balanced_data(df)

    # 3. è®­ç»ƒæ¨¡å‹
    model, metrics = train_balanced_model(X_train, X_test, y_train, y_test)

    # 4. ä¿å­˜æ¨¡å‹
    model_path = save_balanced_model(model, scaler, metrics)

    # 5. è¾“å‡ºæ€»ç»“
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ“Š æ¨¡å‹ä¿¡æ¯:")
    print(f"  è®­ç»ƒæ ·æœ¬: {len(df)}")
    print(f"  ç‰¹å¾ç»´åº¦: {X_train.shape[1]}")
    print(f"  æ¨¡å‹æ€§èƒ½: å‡†ç¡®ç‡ {metrics['accuracy']:.2%}, å¬å›ç‡ {metrics['recall']:.2%}")

    print(f"\nğŸ”§ ä½¿ç”¨æ–¹æ³•:")
    print(f"  1. æ›´æ–°APIé…ç½®ä½¿ç”¨æ¨¡å‹: {model_path}")
    print(f"  2. æµ‹è¯•æ–°æ¨¡å‹æ€§èƒ½")
    print(f"  3. ç‰¹åˆ«æµ‹è¯•ç™¾åº¦ç­‰ä¸­å›½ç½‘ç«™")

if __name__ == "__main__":
    main()