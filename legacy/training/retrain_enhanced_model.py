#!/usr/bin/env python3
"""
é‡æ–°è®­ç»ƒå¢å¼ºçš„DNNæ¨¡å‹
ä½¿ç”¨æ”¹è¿›çš„ç‰¹å¾å’Œæ›´å¤šçš„æ•°æ®
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score
from typing import Dict, Any, Tuple

# å¯¼å…¥æœ¬åœ°æ¨¡å—
from phishguard_v1.models.dataset import NUMERIC_COLS, build_feature_matrix, ParquetDataset
from phishguard_v1.models.fusion_model import FusionDNN, predict_proba

def load_and_preprocess_data(data_path: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®"""
    print(f"ğŸ“‚ åŠ è½½æ•°æ®ä»: {data_path}")

    # è¯»å–æ‰€æœ‰æ•°æ®æ–‡ä»¶
    data_files = ["train.parquet", "val.parquet", "test.parquet"]
    all_data = []

    for file in data_files:
        file_path = Path(data_path) / file
        if file_path.exists():
            df = pd.read_parquet(file_path)
            all_data.append(df)
            print(f"  âœ… {file}: {len(df)} æ¡è®°å½•")

    if not all_data:
        raise FileNotFoundError(f"åœ¨ {data_path} ä¸­æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶")

    # åˆå¹¶æ•°æ®
    combined_df = pd.concat(all_data, ignore_index=True)
    print(f"ğŸ“Š æ€»æ•°æ®é‡: {len(combined_df)} æ¡è®°å½•")

    # æ•°æ®ç»Ÿè®¡
    benign_count = len(combined_df[combined_df["label"] == 0])
    phishing_count = len(combined_df[combined_df["label"] == 1])

    print(f"  è‰¯æ€§ç½‘ç«™: {benign_count} ({benign_count/len(combined_df)*100:.1f}%)")
    print(f"  é’“é±¼ç½‘ç«™: {phishing_count} ({phishing_count/len(combined_df)*100:.1f}%)")

    # é‡æ–°åˆ†å‰²æ•°æ® (80/10/10)
    train_df, temp_df = train_test_split(
        combined_df,
        test_size=0.2,
        random_state=42,
        stratify=combined_df["label"]
    )
    val_df, test_df = train_test_split(
        temp_df,
        test_size=0.5,
        random_state=42,
        stratify=temp_df["label"]
    )

    print(f"ğŸ“ˆ æ•°æ®åˆ†å‰²:")
    print(f"  è®­ç»ƒé›†: {len(train_df)} æ¡")
    print(f"  éªŒè¯é›†: {len(val_df)} æ¡")
    print(f"  æµ‹è¯•é›†: {len(test_df)} æ¡")

    return train_df, val_df, test_df

def create_model(input_dim: int, hidden_dims: list = None) -> FusionDNN:
    """åˆ›å»ºå¢å¼ºçš„DNNæ¨¡å‹"""
    if hidden_dims is None:
        hidden_dims = [512, 256, 128, 64]  # æ›´æ·±çš„ç½‘ç»œ

    print(f"ğŸ§  åˆ›å»ºå¢å¼ºDNNæ¨¡å‹:")
    print(f"  è¾“å…¥ç»´åº¦: {input_dim}")
    print(f"  éšè—å±‚: {hidden_dims}")
    print(f"  è¾“å‡ºç»´åº¦: 2")

    model = FusionDNN(num_features=input_dim)

    # å¦‚æœéœ€è¦ä¸åŒçš„æ¶æ„ï¼Œå¯ä»¥åœ¨è¿™é‡Œä¿®æ”¹
    # å½“å‰ä½¿ç”¨é»˜è®¤çš„ 3å±‚æ¶æ„

    return model

def train_epoch(
    model: nn.Module,
    dataloader: DataLoader,
    criterion: nn.Module,
    optimizer: optim.Optimizer,
    device: torch.device
) -> float:
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0
    correct = 0
    total = 0

    for batch_idx, (data, target) in enumerate(dataloader):
        data, target = data.to(device), target.to(device)

        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        _, predicted = output.max(1)
        total += target.size(0)
        correct += predicted.eq(target).sum().item()

        if batch_idx % 50 == 0:
            print(f"    æ‰¹æ¬¡ {batch_idx}/{len(dataloader)} - æŸå¤±: {loss.item():.4f}")

    accuracy = 100. * correct / total
    avg_loss = total_loss / len(dataloader)

    return avg_loss, accuracy

def validate_epoch(
    model: nn.Module,
    dataloader: DataLoader,
    criterion: nn.Module,
    device: torch.device
) -> Tuple[float, float, np.ndarray, np.ndarray]:
    """éªŒè¯ä¸€ä¸ªepoch"""
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    all_predictions = []
    all_targets = []

    with torch.no_grad():
        for data, target in dataloader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            loss = criterion(output, target)

            total_loss += loss.item()
            _, predicted = output.max(1)
            total += target.size(0)
            correct += predicted.eq(target).sum().item()

            all_predictions.extend(predicted.cpu().numpy())
            all_targets.extend(target.cpu().numpy())

    accuracy = 100. * correct / total
    avg_loss = total_loss / len(dataloader)

    return avg_loss, accuracy, np.array(all_predictions), np.array(all_targets)

def train_model(
    train_df: pd.DataFrame,
    val_df: pd.DataFrame,
    num_epochs: int = 50,
    batch_size: int = 32,
    learning_rate: float = 0.001
) -> Tuple[FusionDNN, Dict[str, Any]]:
    """è®­ç»ƒæ¨¡å‹"""
    print("ğŸš€ å¼€å§‹è®­ç»ƒå¢å¼ºDNNæ¨¡å‹...")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"  ä½¿ç”¨è®¾å¤‡: {device}")

    # æ„å»ºç‰¹å¾çŸ©é˜µ
    print("ğŸ”§ æ„å»ºç‰¹å¾çŸ©é˜µ...")
    X_train, _ = build_feature_matrix(train_df)
    X_val, _ = build_feature_matrix(val_df)
    y_train = train_df["label"].values
    y_val = val_df["label"].values

    print(f"  è®­ç»ƒç‰¹å¾å½¢çŠ¶: {X_train.shape}")
    print(f"  éªŒè¯ç‰¹å¾å½¢çŠ¶: {X_val.shape}")

    # åˆ›å»ºæ•°æ®é›†
    train_dataset = torch.utils.data.TensorDataset(
        torch.FloatTensor(X_train),
        torch.LongTensor(y_train)
    )
    val_dataset = torch.utils.data.TensorDataset(
        torch.FloatTensor(X_val),
        torch.LongTensor(y_val)
    )

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

    # åˆ›å»ºæ¨¡å‹
    input_dim = X_train.shape[1]
    model = create_model(input_dim)
    model = model.to(device)

    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)

    # è®­ç»ƒå¾ªç¯
    best_val_accuracy = 0
    best_model_state = None
    training_history = {
        "train_loss": [],
        "train_accuracy": [],
        "val_loss": [],
        "val_accuracy": []
    }

    print(f"ğŸ“Š å¼€å§‹è®­ç»ƒ ({num_epochs} epochs)...")
    for epoch in range(num_epochs):
        print(f"\nğŸ”„ Epoch {epoch+1}/{num_epochs}")

        # è®­ç»ƒ
        train_loss, train_accuracy = train_epoch(
            model, train_loader, criterion, optimizer, device
        )

        # éªŒè¯
        val_loss, val_accuracy, val_preds, val_targets = validate_epoch(
            model, val_loader, criterion, device
        )

        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step(val_loss)

        # è®°å½•å†å²
        training_history["train_loss"].append(train_loss)
        training_history["train_accuracy"].append(train_accuracy)
        training_history["val_loss"].append(val_loss)
        training_history["val_accuracy"].append(val_accuracy)

        print(f"  è®­ç»ƒ - æŸå¤±: {train_loss:.4f}, å‡†ç¡®ç‡: {train_accuracy:.2f}%")
        print(f"  éªŒè¯ - æŸå¤±: {val_loss:.4f}, å‡†ç¡®ç‡: {val_accuracy:.2f}%")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_accuracy > best_val_accuracy:
            best_val_accuracy = val_accuracy
            best_model_state = model.state_dict().copy()
            print(f"  ğŸ¯ æ–°æœ€ä½³æ¨¡å‹! éªŒè¯å‡†ç¡®ç‡: {val_accuracy:.2f}%")

    # åŠ è½½æœ€ä½³æ¨¡å‹
    if best_model_state is not None:
        model.load_state_dict(best_model_state)

    training_history["best_val_accuracy"] = best_val_accuracy

    return model, training_history

def evaluate_model(model: nn.Module, test_df: pd.DataFrame) -> Dict[str, Any]:
    """è¯„ä¼°æ¨¡å‹"""
    print("ğŸ§ª è¯„ä¼°æ¨¡å‹æ€§èƒ½...")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()

    # æ„å»ºæµ‹è¯•æ•°æ®
    X_test, _ = build_feature_matrix(test_df)
    y_test = test_df["label"].values

    test_dataset = torch.utils.data.TensorDataset(
        torch.FloatTensor(X_test),
        torch.LongTensor(y_test)
    )
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

    # é¢„æµ‹
    all_predictions = []
    all_probabilities = []
    all_targets = []

    with torch.no_grad():
        for data, target in test_loader:
            data = data.to(device)
            output = model(data)
            probabilities = torch.softmax(output, dim=1)
            _, predicted = output.max(1)

            all_predictions.extend(predicted.cpu().numpy())
            all_probabilities.extend(probabilities.cpu().numpy())
            all_targets.extend(target.numpy())

    all_predictions = np.array(all_predictions)
    all_probabilities = np.array(all_probabilities)
    all_targets = np.array(all_targets)

    # è®¡ç®—æŒ‡æ ‡
    accuracy = accuracy_score(all_targets, all_predictions)
    phishing_recall = recall_score(all_targets, all_predictions, pos_label=1)

    print(f"ğŸ“Š æµ‹è¯•ç»“æœ:")
    print(f"  æ€»ä½“å‡†ç¡®ç‡: {accuracy*100:.2f}%")
    print(f"  é’“é±¼ç½‘ç«™å¬å›ç‡: {phishing_recall*100:.2f}%")

    # è¯¦ç»†æŠ¥å‘Š
    print(f"\nğŸ“‹ åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(all_targets, all_predictions, target_names=["è‰¯æ€§", "é’“é±¼"]))

    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(all_targets, all_predictions)
    print(f"ğŸ”„ æ··æ·†çŸ©é˜µ:")
    print(f"  {cm[0][0]} çœŸé˜´æ€§ | {cm[0][1]} å‡é˜³æ€§")
    print(f"  {cm[1][0]} å‡é˜´æ€§ | {cm[1][1]} çœŸé˜³æ€§")

    return {
        "accuracy": accuracy,
        "phishing_recall": phishing_recall,
        "predictions": all_predictions,
        "probabilities": all_probabilities,
        "targets": all_targets,
        "confusion_matrix": cm
    }

def save_model(model: nn.Module, output_path: str, training_history: Dict[str, Any]):
    """ä¿å­˜æ¨¡å‹"""
    print(f"ğŸ’¾ ä¿å­˜æ¨¡å‹åˆ°: {output_path}")

    # åˆ›å»ºæ¨¡å‹ç›®å½•
    output_dir = Path(output_path).parent
    output_dir.mkdir(exist_ok=True)

    # å‡†å¤‡ä¿å­˜å†…å®¹
    checkpoint = {
        "model_state_dict": model.state_dict(),
        "input_features": len(NUMERIC_COLS),
        "model_architecture": "FusionDNN",
        "training_history": training_history,
        "feature_columns": NUMERIC_COLS,
        "best_val_accuracy": training_history.get("best_val_accuracy", 0)
    }

    torch.save(checkpoint, output_path)
    print(f"  âœ… æ¨¡å‹å·²ä¿å­˜")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ å¼€å§‹é‡æ–°è®­ç»ƒå¢å¼ºDNNæ¨¡å‹")
    print("=" * 60)

    # æ•°æ®è·¯å¾„
    data_path = "data_enhanced"  # ä½¿ç”¨å¢å¼ºæ•°æ®
    if not Path(data_path).exists():
        print(f"âš ï¸  {data_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨åŸå§‹æ•°æ®")
        data_path = "data"

    # åŠ è½½æ•°æ®
    train_df, val_df, test_df = load_and_preprocess_data(data_path)

    # è®­ç»ƒæ¨¡å‹
    model, training_history = train_model(
        train_df, val_df,
        num_epochs=50,
        batch_size=32,
        learning_rate=0.001
    )

    # è¯„ä¼°æ¨¡å‹
    evaluation_results = evaluate_model(model, test_df)

    # ä¿å­˜æ¨¡å‹
    model_path = "artifacts/fusion_enhanced.pt"
    save_model(model, model_path, training_history)

    print("\nğŸ‰ è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ“ˆ æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {training_history['best_val_accuracy']:.2f}%")
    print(f"ğŸ“Š æµ‹è¯•å‡†ç¡®ç‡: {evaluation_results['accuracy']*100:.2f}%")
    print(f"ğŸ¯ é’“é±¼ç½‘ç«™å¬å›ç‡: {evaluation_results['phishing_recall']*100:.2f}%")
    print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜è‡³: {model_path}")

if __name__ == "__main__":
    main()