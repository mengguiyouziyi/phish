#!/usr/bin/env python3
"""
åˆå¹¶GitHubéªŒè¯æ•°æ®ä¸ç°æœ‰æ•°æ®ï¼Œé‡æ–°è®­ç»ƒèåˆæ¨¡å‹
"""

import pandas as pd
import numpy as np
from pathlib import Path
import torch
import torch.nn as nn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import json
import time
import sys
sys.path.append('.')

from phishguard_v1.models.fusion_model import FusionDNN
from phishguard_v1.models.dataset import NUMERIC_COLS

def load_existing_data():
    """åŠ è½½ç°æœ‰æ•°æ®"""
    data_files = [
        "data/enhanced_dataset.parquet",
        "artifacts/fusion_training_data.parquet",
        "data/training_data.parquet",
        "data/dataset.parquet"
    ]

    for file_path in data_files:
        if Path(file_path).exists():
            try:
                print(f"ğŸ“¦ å°è¯•åŠ è½½ç°æœ‰æ•°æ®: {file_path}")
                df = pd.read_parquet(file_path)
                print(f"  æˆåŠŸåŠ è½½: {len(df)} æ¡è®°å½•")
                return df
            except Exception as e:
                print(f"  âŒ åŠ è½½å¤±è´¥: {e}")
                continue

    print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„ç°æœ‰è®­ç»ƒæ•°æ®")
    return None

def load_github_data():
    """åŠ è½½GitHubéªŒè¯æ•°æ®"""
    github_file = "validated_github_data/training_data.parquet"
    if Path(github_file).exists():
        print(f"ğŸ“¦ åŠ è½½GitHubæ•°æ®: {github_file}")
        return pd.read_parquet(github_file)
    else:
        print("âŒ æœªæ‰¾åˆ°GitHubéªŒè¯æ•°æ®")
        return None

def merge_datasets(existing_df, github_df):
    """åˆå¹¶æ•°æ®é›†"""
    print(f"ğŸ”„ åˆå¹¶æ•°æ®é›†...")
    print(f"  ç°æœ‰æ•°æ®: {len(existing_df)} æ¡è®°å½•")
    print(f"  GitHubæ•°æ®: {len(github_df)} æ¡è®°å½•")

    # ç¡®ä¿åˆ—åä¸€è‡´
    existing_df = existing_df.copy()
    github_df = github_df.copy()

    # é‡å‘½åGitHubæ•°æ®çš„åˆ—ä»¥åŒ¹é…ç°æœ‰æ ¼å¼
    column_mapping = {
        'url': 'url',
        'final_url': 'final_url',
        'label': 'label',
        'timestamp': 'timestamp'
    }

    # åªä¿ç•™å…±æœ‰çš„ç‰¹å¾åˆ—
    common_features = set(existing_df.columns) & set(github_df.columns)
    common_features = [col for col in common_features if col in NUMERIC_COLS or col in column_mapping.values()]

    print(f"  å…±åŒç‰¹å¾æ•°: {len([f for f in common_features if f in NUMERIC_COLS])}")

    # æå–å…±åŒç‰¹å¾
    existing_features = existing_df[common_features].copy()
    github_features = github_df[common_features].copy()

    # åˆå¹¶æ•°æ®
    merged_df = pd.concat([existing_features, github_features], ignore_index=True)

    # å»é‡ï¼ˆåŸºäºURLï¼‰
    initial_count = len(merged_df)
    merged_df = merged_df.drop_duplicates(subset=['url'], keep='first')
    final_count = len(merged_df)

    print(f"  åˆå¹¶åè®°å½•: {final_count} æ¡")
    print(f"  å»é‡åå‡å°‘: {initial_count - final_count} æ¡")

    return merged_df

def prepare_training_data(df):
    """å‡†å¤‡è®­ç»ƒæ•°æ®"""
    print("ğŸ”§ å‡†å¤‡è®­ç»ƒæ•°æ®...")

    # ç¡®ä¿æ‰€æœ‰å¿…éœ€çš„ç‰¹å¾åˆ—éƒ½å­˜åœ¨
    missing_features = [col for col in NUMERIC_COLS if col not in df.columns]
    if missing_features:
        print(f"âš ï¸  ç¼ºå¤±ç‰¹å¾: {missing_features}")
        # ä¸ºç¼ºå¤±ç‰¹å¾æ·»åŠ é»˜è®¤å€¼
        for col in missing_features:
            df[col] = 0.0

    # é€‰æ‹©ç‰¹å¾åˆ—å’Œæ ‡ç­¾
    feature_cols = [col for col in NUMERIC_COLS if col in df.columns]
    X = df[feature_cols].values
    y = df['label'].values

    # ç§»é™¤åŒ…å«NaNçš„è¡Œ
    valid_mask = ~np.isnan(X).any(axis=1)
    X = X[valid_mask]
    y = y[valid_mask]

    print(f"  æœ‰æ•ˆæ ·æœ¬æ•°: {len(X)}")
    print(f"  ç‰¹å¾ç»´åº¦: {X.shape[1]}")
    print(f"  é’“é±¼ç½‘ç«™: {np.sum(y == 1)} ä¸ª")
    print(f"  è‰¯æ€§ç½‘ç«™: {np.sum(y == 0)} ä¸ª")

    # æ•°æ®åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # æ•°æ®æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    return X_train_scaled, X_test_scaled, y_train, y_test, scaler, feature_cols

def train_fusion_model(X_train, X_test, y_train, y_test, feature_cols):
    """è®­ç»ƒèåˆæ¨¡å‹"""
    print("ğŸ§  è®­ç»ƒèåˆæ¨¡å‹...")

    # è®¾å¤‡é…ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"  ä½¿ç”¨è®¾å¤‡: {device}")

    # åˆ›å»ºæ¨¡å‹
    input_dim = X_train.shape[1]
    model = FusionDNN(num_features=input_dim).to(device)

    # è½¬æ¢æ•°æ®ä¸ºå¼ é‡
    X_train_tensor = torch.FloatTensor(X_train).to(device)
    y_train_tensor = torch.FloatTensor(y_train).to(device)
    X_test_tensor = torch.FloatTensor(X_test).to(device)
    y_test_tensor = torch.FloatTensor(y_test).to(device)

    # è®­ç»ƒå‚æ•°
    num_epochs = 100
    batch_size = 64
    learning_rate = 0.001

    # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    criterion = nn.BCELoss().to(device)  # å°†æŸå¤±å‡½æ•°ç§»åŠ¨åˆ°è®¾å¤‡

    # è®­ç»ƒå¾ªç¯
    model.train()
    print(f"  å¼€å§‹è®­ç»ƒï¼Œå…± {num_epochs} è½®...")

    for epoch in range(num_epochs):
        # å°æ‰¹é‡è®­ç»ƒ
        total_loss = 0
        num_batches = 0

        for i in range(0, len(X_train_tensor), batch_size):
            batch_X = X_train_tensor[i:i + batch_size]
            batch_y = y_train_tensor[i:i + batch_size]

            optimizer.zero_grad()
            outputs = model(batch_X).squeeze()
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            num_batches += 1

        avg_loss = total_loss / num_batches

        if (epoch + 1) % 20 == 0:
            print(f"    è½®æ¬¡ {epoch+1}/{num_epochs}, å¹³å‡æŸå¤±: {avg_loss:.4f}")

    # è¯„ä¼°æ¨¡å‹
    model.eval()
    with torch.no_grad():
        test_outputs = model(X_test_tensor).squeeze()
        test_predictions = (test_outputs >= 0.5).float()

        test_accuracy = accuracy_score(y_test, test_predictions.cpu().numpy())
        test_precision = precision_score(y_test, test_predictions.cpu().numpy(), zero_division=0)
        test_recall = recall_score(y_test, test_predictions.cpu().numpy(), zero_division=0)
        test_f1 = f1_score(y_test, test_predictions.cpu().numpy(), zero_division=0)

        print(f"  æµ‹è¯•é›†æ€§èƒ½:")
        print(f"    å‡†ç¡®ç‡: {test_accuracy:.4f}")
        print(f"    ç²¾ç¡®ç‡: {test_precision:.4f}")
        print(f"    å¬å›ç‡: {test_recall:.4f}")
        print(f"    F1åˆ†æ•°: {test_f1:.4f}")

    return model, scaler, {
        'accuracy': test_accuracy,
        'precision': test_precision,
        'recall': test_recall,
        'f1': test_f1
    }

def save_model(model, scaler, feature_cols, metrics):
    """ä¿å­˜æ¨¡å‹"""
    print("ğŸ’¾ ä¿å­˜æ¨¡å‹...")

    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = Path("artifacts")
    save_dir.mkdir(exist_ok=True)

    # ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹
    checkpoint = {
        'model_state_dict': model.state_dict(),
        'input_features': len(feature_cols),
        'feature_names': feature_cols,
        'scaler_mean': scaler.mean_.tolist(),
        'scaler_scale': scaler.scale_.tolist(),
        'metrics': metrics,
        'training_time': time.time()
    }

    model_path = save_dir / "fusion_github_enhanced.pt"
    torch.save(checkpoint, model_path)
    print(f"  æ¨¡å‹å·²ä¿å­˜: {model_path}")

    # ä¿å­˜è®­ç»ƒæ•°æ®
    training_data_path = save_dir / "fusion_github_training_data.parquet"
    return model_path, training_data_path

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹åˆå¹¶æ•°æ®å¹¶é‡æ–°è®­ç»ƒèåˆæ¨¡å‹")
    print("=" * 60)

    # 1. åŠ è½½æ•°æ®
    existing_df = load_existing_data()
    github_df = load_github_data()

    if existing_df is None and github_df is None:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è®­ç»ƒæ•°æ®ï¼Œé€€å‡º")
        return

    if existing_df is None:
        print("âš ï¸  ä»…ä½¿ç”¨GitHubæ•°æ®è®­ç»ƒæ–°æ¨¡å‹")
        merged_df = github_df
    elif github_df is None:
        print("âš ï¸  æœªæ‰¾åˆ°GitHubæ•°æ®ï¼Œä»…ä½¿ç”¨ç°æœ‰æ•°æ®è®­ç»ƒ")
        merged_df = existing_df
    else:
        # 2. åˆå¹¶æ•°æ®
        merged_df = merge_datasets(existing_df, github_df)

    # 3. å‡†å¤‡è®­ç»ƒæ•°æ®
    X_train, X_test, y_train, y_test, scaler, feature_cols = prepare_training_data(merged_df)

    # 4. è®­ç»ƒæ¨¡å‹
    model, scaler, metrics = train_fusion_model(X_train, X_test, y_train, y_test, feature_cols)

    # 5. ä¿å­˜æ¨¡å‹
    model_path, data_path = save_model(model, scaler, feature_cols, metrics)

    # 6. ä¿å­˜åˆå¹¶åçš„æ•°æ®
    merged_data_path = "artifacts/fusion_github_merged_data.parquet"
    merged_df.to_parquet(merged_data_path)
    print(f"  åˆå¹¶æ•°æ®å·²ä¿å­˜: {merged_data_path}")

    # 7. è¾“å‡ºæ€»ç»“
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
    print(f"  æ€»è®­ç»ƒæ ·æœ¬: {len(merged_df)}")
    print(f"  ç‰¹å¾ç»´åº¦: {len(feature_cols)}")
    print(f"  æ¨¡å‹æ€§èƒ½: å‡†ç¡®ç‡ {metrics['accuracy']:.2%}, å¬å›ç‡ {metrics['recall']:.2%}")
    print(f"  æ¨¡å‹æ–‡ä»¶: {model_path}")
    print(f"  æ•°æ®æ–‡ä»¶: {merged_data_path}")

    print(f"\nğŸ”§ ä¸‹ä¸€æ­¥æ“ä½œ:")
    print(f"  1. æ›´æ–°APIé…ç½®ä½¿ç”¨æ–°æ¨¡å‹: fusion_github_enhanced.pt")
    print(f"  2. æµ‹è¯•æ¨¡å‹æ€§èƒ½")
    print(f"  3. è¯„ä¼°æ”¹è¿›æ•ˆæœ")

if __name__ == "__main__":
    main()