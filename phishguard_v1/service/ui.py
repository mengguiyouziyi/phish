from __future__ import annotations
import argparse
import asyncio
import csv
import json
import os
import uuid
from datetime import datetime
from typing import Any, Dict, List, Tuple
import tempfile
from pathlib import Path

import gradio as gr
import pandas as pd
from httpx import AsyncClient

try:
    from .ui_patch import DataFrame as CompatDataFrame
except Exception:  # pragma: no cover - ç›´æ¥è¿è¡ŒåŒ…æ—¶çš„å¯¼å…¥
    from phishguard_v1.service.ui_patch import DataFrame as CompatDataFrame

from ..config import settings
from ..features.fetcher import fetch_one
from ..features.parser import extract_from_html
from ..features.render import render_screenshot
from ..models.inference import InferencePipeline

pipe = InferencePipeline(fusion_ckpt_path="artifacts/fusion_dalwfr_v5.pt", enable_fusion=True)

SIGNAL_FEATURES = {
    "suspicious_js_inline": "å¯ç–‘å†…è”è„šæœ¬",
    "external_form_ratio": "å¤–éƒ¨è¡¨å•å æ¯”",
    "hidden_input_ratio": "éšè—å­—æ®µå¯†åº¦",
    "http_security_header_count": "å®‰å…¨å“åº”å¤´æ•°é‡",
    "http_tls_retry_flag": "TLSå›é€€è§¦å‘",
    "cookie_secure_ratio": "Secure Cookie å æ¯”",
    "cookie_httponly_ratio": "HttpOnly Cookie å æ¯”",
    "meta_sensitive_kw_flag": "æ•æ„Ÿå…³é”®è¯æ ‡è®°",
    "title_entropy": "æ ‡é¢˜ç†µ",
    "fingerprint_hash_len": "æŒ‡çº¹å“ˆå¸Œé•¿åº¦",
}


def _md_escape(value: Any) -> str:
    text = str(value) if value is not None else ""
    return text.replace("|", "\\|").replace("\n", " ")


def build_http_summary_block(features: Dict[str, Any]) -> str:
    lines = ["### ğŸŒ HTTP ä¿¡æ¯"]
    status = features.get("status_code")
    if status is not None:
        lines.append(f"- **çŠ¶æ€ç **ï¼š{status}")
    content_type = features.get("content_type")
    if content_type:
        lines.append(f"- **Content-Type**ï¼š{content_type}")
    redirects = (features.get("meta") or {}).get("redirects") or []
    if redirects:
        lines.append(f"- **é‡å®šå‘é“¾è·¯**ï¼š{' â†’ '.join(_md_escape(r) for r in redirects[:5])}")

    headers = features.get("headers") or {}
    if headers:
        lines.append("\n| Header | Value |\n| --- | --- |")
        for key, value in list(headers.items())[:10]:
            lines.append(f"| {_md_escape(key)} | {_md_escape(value)} |")
    else:
        lines.append("- æœªè·å–åˆ°å“åº”å¤´ (ç«™ç‚¹å¯èƒ½æ‹’ç»è¿æ¥æˆ–é‡å®šå‘è‡³é HTTP é¡µé¢)ã€‚")
    return "\n".join(lines)


def build_cookie_summary_block(features: Dict[str, Any]) -> str:
    lines = ["### ğŸª Cookie ä¿¡æ¯"]
    cookies = features.get("cookies") or {}
    set_cookie = features.get("set_cookie") or ""
    if cookies:
        lines.append(f"- **Cookie æ€»æ•°**ï¼š{len(cookies)}")
        lines.append("| Cookie | å€¼ |\n| --- | --- |")
        for key, value in list(cookies.items())[:10]:
            lines.append(f"| {_md_escape(key)} | {_md_escape(value)} |")
    else:
        lines.append("- æœªæ£€æµ‹åˆ°å“åº” Cookieã€‚")
    if set_cookie:
        preview = _md_escape(set_cookie[:300]) + ("â€¦" if len(set_cookie) > 300 else "")
        lines.append(f"- **Set-Cookie åŸå§‹ä¸²ï¼ˆæˆªæ–­ï¼‰**ï¼š`{preview}`")
    return "\n".join(lines)


def build_meta_summary_block(features: Dict[str, Any]) -> str:
    lines = ["### ğŸ§© Meta / æŒ‡çº¹ä¿¡æ¯"]
    html_feats = features.get("html_feats") or {}
    meta_kv = html_feats.get("meta_kv") or {}
    if meta_kv:
        lines.append("| Meta åç§° | å†…å®¹ |\n| --- | --- |")
        for key, value in list(meta_kv.items())[:10]:
            lines.append(f"| {_md_escape(key)} | {_md_escape(value)} |")
    else:
        lines.append("- æœªæå–åˆ° Meta æ ‡ç­¾ã€‚")

    script_srcs = html_feats.get("script_srcs") or []
    stylesheets = html_feats.get("stylesheets") or []
    if script_srcs or stylesheets:
        lines.append("\n- **å¤–éƒ¨è„šæœ¬**ï¼š")
        lines.extend([f"  - {_md_escape(src)}" for src in script_srcs[:5]])
        lines.append("- **å¤–éƒ¨æ ·å¼è¡¨**ï¼š")
        lines.extend([f"  - {_md_escape(href)}" for href in stylesheets[:5]])
    return "\n".join(lines)


def format_probability(prob: float) -> str:
    return f"{prob * 100:.2f}%"


def get_risk_level(prob: float) -> Tuple[str, str]:
    if prob >= 0.9:
        return "ğŸ”´ é«˜é£é™©", "#f44336"
    if prob >= 0.7:
        return "ğŸŸ¡ ä¸­é£é™©", "#ff9800"
    if prob >= 0.5:
        return "ğŸŸ  ä½é£é™©", "#ffc107"
    return "ğŸŸ¢ å®‰å…¨", "#4caf50"


def generate_conclusion(pred: Dict[str, Any]) -> str:
    url_prob = pred.get("url_prob", 0)
    fusion_prob = pred.get("fusion_prob")
    final_prob = pred.get("final_prob", 0)
    label = pred.get("label", 0)

    parts = []
    parts.append("ğŸš¨ **æ£€æµ‹ç»“æœï¼šé’“é±¼ç½‘ç«™**" if label == 1 else "âœ… **æ£€æµ‹ç»“æœï¼šè‰¯æ€§ç½‘ç«™**")
    risk_level, _ = get_risk_level(final_prob)
    parts.append(f"ğŸ“Š **é£é™©ç­‰çº§ï¼š{risk_level}** ({format_probability(final_prob)})")

    parts.append("ğŸ¤– **æ¨¡å‹åˆ†æï¼š**")
    parts.append(f"   - URL é¢„è®­ç»ƒæ¨¡å‹ï¼š{format_probability(url_prob)}")
    if fusion_prob is not None:
        parts.append(f"   - FusionDNN æ¨¡å‹ï¼š{format_probability(fusion_prob)}")
    else:
        parts.append("   - FusionDNN æ¨¡å‹ï¼šæœªå‚ä¸èåˆ")

    parts.append("âš ï¸ **å»ºè®®ï¼š** é¿å…è®¿é—®æ­¤ç½‘ç«™ï¼Œå¯èƒ½å­˜åœ¨å®‰å…¨é£é™©" if label == 1 else "ğŸ’¡ **å»ºè®®ï¼š** ç½‘ç«™çœ‹èµ·æ¥å®‰å…¨ï¼Œä½†ä»éœ€ä¿æŒè­¦æƒ•")
    return "\n\n".join(parts)


def build_probability_summary(pred: Dict[str, Any]) -> str:
    rows = [
        ("URL æ¨¡å‹", pred.get("url_prob")),
        ("FusionDNN æ¨¡å‹", pred.get("fusion_prob")),
        ("ç»¼åˆç»“æœ", pred.get("final_prob")),
    ]
    lines = ["### æ¦‚ç‡æ‹†è§£", "| æ¨¡å— | é’“é±¼æ¦‚ç‡ |", "| --- | --- |"]
    for name, prob in rows:
        lines.append(f"| {name} | {format_probability(prob)} |" if prob is not None else f"| {name} | N/A |")
    return "\n".join(lines)


def build_detail_summary(details: Dict[str, Any] | None) -> str:
    if not details:
        return "### æ¨ç†ç»†èŠ‚\n- æš‚æ— æ¨ç†ç»†èŠ‚ä¿¡æ¯ã€‚"

    lines = ["### æ¨ç†ç»†èŠ‚"]
    decision = details.get("decision")
    if decision:
        lines.append(f"- **èåˆç­–ç•¥**ï¼š{decision}")

    weights = details.get("fusion_weights", {})
    if weights:
        lines.append("- **æƒé‡åˆ†é…ï¼š**")
        lines.append(f"  - URL æ¨¡å‹ï¼š{weights.get('url', 0) * 100:.1f}%")
        lines.append(f"  - FusionDNNï¼š{weights.get('fusion', 0) * 100:.1f}%")

    thresholds = details.get("thresholds", {})
    if thresholds:
        lines.append("- **åˆ¤å®šé˜ˆå€¼ï¼š**")
        lines.extend(
            [
                f"  - {key}ï¼š{value:.2f}" if isinstance(value, (int, float)) else f"  - {key}ï¼š{value}"
                for key, value in thresholds.items()
            ]
        )

    snapshot = details.get("feature_snapshot")
    if snapshot:
        contributions = []
        for key, label in SIGNAL_FEATURES.items():
            val = snapshot.get(key)
            if val is None:
                continue
            magnitude = abs(float(val))
            if magnitude <= 0:
                continue
            contributions.append((magnitude, label, float(val)))
        if contributions:
            contributions.sort(reverse=True)
            lines.append("- **å…³é”®æŒ‡çº¹ç‰¹å¾ï¼š**")
            for _, label, value in contributions[:5]:
                lines.append(f"  - {label}ï¼š{value:.2f}")
    return "\n".join(lines)


def create_feature_popup(features: Dict[str, Any]) -> str:
    popup_content: List[str] = []
    url_feats = features.get("url_feats", {})
    if url_feats:
        popup_content.append("### ğŸ”— URL ç‰¹å¾")
        popup_content.append(f"- **æ€»é•¿åº¦**ï¼š{url_feats.get('url_len', 0)}")
        popup_content.append(f"- **åŸŸåé•¿åº¦**ï¼š{url_feats.get('host_len', 0)}")
        popup_content.append(f"- **è·¯å¾„é•¿åº¦**ï¼š{url_feats.get('path_len', 0)}")
        popup_content.append(f"- **æ•°å­—å­—ç¬¦æ•°**ï¼š{url_feats.get('num_digits', 0)}")
        popup_content.append(f"- **ç‰¹æ®Šå­—ç¬¦æ•°**ï¼š{url_feats.get('num_specials', 0)}")
        popup_content.append(f"- **å­åŸŸåæ·±åº¦**ï¼š{url_feats.get('subdomain_depth', 0)}")
        popup_content.append(f"- **æ˜¯å¦åŒ…å« IP**ï¼š{'æ˜¯' if url_feats.get('has_ip') else 'å¦'}")
        popup_content.append(f"- **åè®®**ï¼š{'HTTPS' if url_feats.get('scheme_https') else 'HTTP'}")

    html_feats = features.get("html_feats", {})
    if html_feats:
        popup_content.append("\n### ğŸ“„ HTML ç‰¹å¾")
        popup_content.append(f"- **æ ‡é¢˜é•¿åº¦**ï¼š{html_feats.get('title_len', 0)}")
        popup_content.append(f"- **å…ƒæ ‡ç­¾æ•°**ï¼š{html_feats.get('num_meta', 0)}")
        popup_content.append(f"- **é“¾æ¥æ•°**ï¼š{html_feats.get('num_links', 0)}")
        popup_content.append(f"- **è„šæœ¬æ•°**ï¼š{html_feats.get('num_scripts', 0)}")
        popup_content.append(f"- **è¡¨å•æ•°**ï¼š{html_feats.get('num_forms', 0)}")
        popup_content.append(f"- **æ˜¯å¦æœ‰å¯†ç è¾“å…¥**ï¼š{'æ˜¯' if html_feats.get('has_password_input') else 'å¦'}")
        popup_content.append(f"- **å¯ç–‘è„šæœ¬**ï¼š{'æ˜¯' if html_feats.get('suspicious_js_inline') else 'å¦'}")

    status_code = features.get("status_code")
    content_type = features.get("content_type")
    bytes_size = features.get("bytes")
    if status_code or content_type or bytes_size:
        popup_content.append("\n### ğŸŒ HTTP å“åº”ç‰¹å¾")
        if status_code:
            popup_content.append(f"- **çŠ¶æ€ç **ï¼š{status_code}")
        if content_type:
            popup_content.append(f"- **å†…å®¹ç±»å‹**ï¼š{content_type}")
        if bytes_size:
            popup_content.append(f"- **å“åº”å¤§å°**ï¼š{bytes_size} bytes")

    return "\n".join(popup_content) if popup_content else "### ç‰¹å¾æ‘˜è¦\n- æš‚æ— ç‰¹å¾ä¿¡æ¯ã€‚"


def build_history_rows(history: List[Dict[str, Any]]) -> List[List[str]]:
    rows = []
    for item in history:
        rows.append([item["time"], item["url"], item["probability"], item["label_text"]])
    return rows


def build_batch_results(results: List[Any]) -> Tuple[List[List[str]], str, Dict[str, int]]:
    headers = ["URL", "æ£€æµ‹ç»“æœ", "é£é™©ç­‰çº§", "URL æ¨¡å‹", "FusionDNN", "ç»¼åˆæ¦‚ç‡", "å¤‡æ³¨"]
    rows: List[List[str]] = []
    tmp_file = tempfile.NamedTemporaryFile(
        prefix="phish_batch_",
        suffix=".csv",
        delete=False,
        dir=tempfile.gettempdir(),
    )
    csv_path = Path(tmp_file.name)
    phish_count = 0
    error_count = 0

    for idx, result in enumerate(results, start=1):
        if isinstance(result, Exception):
            rows.append([f"ä»»åŠ¡_{idx}", "å¤±è´¥", "-", "-", "-", "-", str(result)])
            error_count += 1
            continue

        pred = result.get("prediction", {})
        features = result.get("features", {})
        final_url = features.get("final_url") or features.get("request_url", f"URL_{idx}")
        final_prob = pred.get("final_prob", 0.0)
        label = pred.get("label", 0)
        result_text = "é’“é±¼" if label == 1 else "è‰¯æ€§"
        risk_level, _ = get_risk_level(final_prob)
        if label == 1:
            phish_count += 1

        url_prob = format_probability(pred.get("url_prob", 0.0))
        fusion_prob_val = pred.get("fusion_prob")
        fusion_prob = format_probability(fusion_prob_val) if fusion_prob_val is not None else "N/A"
        final_prob_str = format_probability(final_prob)
        rows.append([final_url, result_text, risk_level, url_prob, fusion_prob, final_prob_str, ""])

    with open(csv_path, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(headers)
        for line in rows:
            writer.writerow(line)

    stats = {"total": len(results), "phish": phish_count, "errors": error_count}
    return rows, str(csv_path), stats


async def scan_single(url: str, screenshot: bool) -> Dict[str, Any]:
    # URLè¾“å…¥éªŒè¯å’Œé”™è¯¯å¤„ç†
    if not url or not url.strip():
        raise ValueError("è¯·è¾“å…¥æœ‰æ•ˆçš„URL")

    # åŸºæœ¬URLæ ¼å¼éªŒè¯
    url = url.strip()
    if not (url.startswith('http://') or url.startswith('https://')):
        # å°è¯•è‡ªåŠ¨æ·»åŠ https://å‰ç¼€
        if '://' not in url:
            url = 'https://' + url
        else:
            raise ValueError("URLå¿…é¡»ä»¥http://æˆ–https://å¼€å¤´")

    try:
        async with AsyncClient(timeout=settings.http_timeout, headers={"User-Agent": settings.user_agent}) as client:
            item = await fetch_one(url, client)

        if not item or not isinstance(item, dict):
            item = {
                "request_url": url,
                "final_url": url,
                "status_code": None,
                "content_type": None,
                "headers": {},
                "html": "",
                "bytes": 0,
                "url_feats": {},
                "ok": False,
                "meta": {"ok": False, "error": "æŠ“å–å¤±è´¥", "redirects": []},
            }

        html_content = item.get("html", "")
        final_url = item.get("final_url") or item.get("request_url", url)
        item["html_feats"] = extract_from_html(html_content, final_url)

        if screenshot:
            snap = render_screenshot(final_url)
            item.update(snap)

        if not item.get("url_feats"):
            from ..features.url_features import url_stats

            item["url_feats"] = url_stats(final_url)

        pred = pipe.predict(item)
        return {"prediction": pred, "features": item}
    except Exception as exc:
        error_item = {
            "request_url": url,
            "final_url": url,
            "status_code": None,
            "content_type": None,
            "headers": {},
            "html": "",
            "bytes": 0,
            "url_feats": {},
            "html_feats": {},
            "ok": False,
            "meta": {"ok": False, "error": str(exc), "redirects": []},
        }
        try:
            pred = pipe.predict(error_item)
            return {"prediction": pred, "features": error_item}
        except Exception:
            raise exc


async def scan_multiple(urls_text: str, screenshot: bool) -> List[Any]:
    # æ‰¹é‡URLè¾“å…¥éªŒè¯
    urls = []
    invalid_urls = []

    for line in urls_text.splitlines():
        url = line.strip()
        if not url:
            continue

        # åŸºæœ¬URLæ ¼å¼éªŒè¯
        if not (url.startswith('http://') or url.startswith('https://')):
            if '://' not in url:
                url = 'https://' + url
            else:
                invalid_urls.append(line)
                continue

        urls.append(url)

    if not urls:
        if invalid_urls:
            raise ValueError(f"æ£€æµ‹åˆ° {len(invalid_urls)} ä¸ªæ— æ•ˆçš„URLæ ¼å¼")
        return []

    tasks = [scan_single(url, screenshot) for url in urls]
    return await asyncio.gather(*tasks, return_exceptions=True)


def update_single_result(result: Any, history: List[Dict[str, Any]]) -> Tuple[
    str,
    str,
    str,
    str,
    Dict[str, Any],
    Dict[str, Any],
    str,
    gr.Update,
    str,
    str,
    str,
    gr.Update,
    List[Dict[str, Any]],
]:
    history = list(history or [])
    history_rows = build_history_rows(history)

    if isinstance(result, Exception):
        conclusion = f"âŒ æ£€æµ‹å¤±è´¥ï¼š{result}"
        status_html = (
            "<div style=\"text-align:center;padding:20px;background-color:#f4433620;border-radius:8px;\">"
            "<div style=\"font-size:24px;margin-bottom:8px;\">æ£€æµ‹å¤±è´¥</div>"
            "<div style=\"font-size:16px;\">è¯·ç¨åé‡è¯•æˆ–æ£€æŸ¥ç½‘ç»œ</div>"
            "</div>"
        )
        prob_summary = "### æ¦‚ç‡æ‹†è§£\n- æ£€æµ‹å¤±è´¥ï¼Œæš‚æ— æ¦‚ç‡ä¿¡æ¯ã€‚"
        detail_summary = "### æ¨ç†ç»†èŠ‚\n- æ£€æµ‹å¤±è´¥ï¼Œæš‚æ— æ¨ç†ç»†èŠ‚ã€‚"
        features_text = "### ç‰¹å¾æ‘˜è¦\n- æš‚æ— ç‰¹å¾ä¿¡æ¯ã€‚"
        http_summary = "### ğŸŒ HTTP ä¿¡æ¯\n- æ£€æµ‹å¤±è´¥ï¼Œæš‚æ— æ•°æ®ã€‚"
        cookie_summary = "### ğŸª Cookie ä¿¡æ¯\n- æ£€æµ‹å¤±è´¥ï¼Œæš‚æ— æ•°æ®ã€‚"
        meta_summary = "### ğŸ§© Meta / æŒ‡çº¹ä¿¡æ¯\n- æ£€æµ‹å¤±è´¥ï¼Œæš‚æ— æ•°æ®ã€‚"
        return (
            conclusion,
            status_html,
            prob_summary,
            detail_summary,
            {},
            {},
            features_text,
            http_summary,
            cookie_summary,
            meta_summary,
            gr.update(value=None, visible=False),
            "",
            "",
            "",
            gr.update(value=history_rows),
            history,
        )

    pred = result.get("prediction", {}) or {}
    features = result.get("features", {}) or {}

    conclusion = generate_conclusion(pred)
    final_prob = pred.get("final_prob", 0.0)
    risk_level, color = get_risk_level(final_prob)
    status_html = (
        "<div style=\"text-align:center;padding:20px;border-radius:8px;\" "
        f"style=\"background-color:{color}20\">"
        f"<div style=\"font-size:24px;margin-bottom:8px;\">{risk_level}</div>"
        f"<div style=\"font-size:20px;font-weight:bold;\">{format_probability(final_prob)}</div>"
        f"<div style=\"margin-top:8px;\">{'ğŸš¨ é’“é±¼ç½‘ç«™' if pred.get('label', 0) == 1 else 'âœ… è‰¯æ€§ç½‘ç«™'}</div>"
        "</div>"
    )

    prob_summary = build_probability_summary(pred)
    detail_summary = build_detail_summary(pred.get("details"))
    features_text = create_feature_popup(features)
    http_summary = build_http_summary_block(features)
    cookie_summary = build_cookie_summary_block(features)
    meta_summary = build_meta_summary_block(features)

    final_url = features.get("final_url") or features.get("request_url", "")
    status_code_val = str(features.get("status_code", "")) if features.get("status_code") else ""
    content_type_val = features.get("content_type", "") or ""

    screenshot_path = features.get("screenshot_path")
    screenshot_update = gr.update(value=screenshot_path, visible=bool(screenshot_path))

    history_entry = {
        "time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "url": final_url,
        "probability": format_probability(final_prob),
        "label_text": "é’“é±¼" if pred.get("label", 0) == 1 else "è‰¯æ€§",
    }
    history.insert(0, history_entry)
    history = history[:50]
    history_rows = build_history_rows(history)

    return (
        conclusion,
        status_html,
        prob_summary,
        detail_summary,
        pred,
        pred.get("details", {}),
        features_text,
        http_summary,
        cookie_summary,
        meta_summary,
        screenshot_update,
        final_url,
        status_code_val,
        content_type_val,
        gr.update(value=history_rows),
        history,
    )


def build_interface():
    with gr.Blocks(
        title="PhishGuard v1",
        theme=gr.themes.Soft(),
        css="""
        .main-container {max-width: 1180px; margin: auto;}
        """,
    ) as demo:
        history_state = gr.State([])
        gr.Markdown(
            """
            # ğŸ›¡ï¸ PhishGuard v1 - é«˜çº§é’“é±¼ç½‘ç«™æ£€æµ‹ç³»ç»Ÿ

            - ğŸ¤– **URL é¢„è®­ç»ƒæ¨¡å‹**ï¼šå³æ—¶è¯­ä¹‰ç†è§£
            - ğŸ§  **FusionDNN æ¨¡å‹**ï¼šå¤šç‰¹å¾æ·±åº¦èåˆ
            - ğŸ“Š **å…¨é“¾è·¯è¯Šæ–­**ï¼šæ¦‚ç‡æ‹†è§£ + å†³ç­–ç»†èŠ‚ + å†å²è¿½è¸ª
            """
        )

        with gr.Tabs():
            with gr.TabItem("ğŸ” å• URL æ£€æµ‹"):
                with gr.Row():
                    with gr.Column(scale=3):
                        url_input = gr.Textbox(
                            label="è¾“å…¥è¦æ£€æµ‹çš„ URL",
                            placeholder="https://example.com",
                            value="https://example.com",
                        )
                    with gr.Column(scale=1):
                        screenshot_cb = gr.Checkbox(label="å¯ç”¨æˆªå›¾åŠŸèƒ½", value=False)
                        scan_btn = gr.Button("ğŸ” å¼€å§‹æ£€æµ‹", variant="primary")

                with gr.Row():
                    with gr.Column(scale=2):
                        conclusion_box = gr.Markdown(value="è¯·è¾“å…¥ URL å¹¶ç‚¹å‡»æ£€æµ‹")
                    with gr.Column(scale=1):
                        status_indicator = gr.HTML("<div style='text-align:center;padding:20px;'>â³ ç­‰å¾…æ£€æµ‹...</div>")

                with gr.Row():
                    probability_summary = gr.Markdown("### æ¦‚ç‡æ‹†è§£\n- ç­‰å¾…æ£€æµ‹")
                    detail_summary = gr.Markdown("### æ¨ç†ç»†èŠ‚\n- ç­‰å¾…æ£€æµ‹")

                with gr.Accordion("ğŸ“Š è¯¦ç»†åˆ†æç»“æœ", open=False):
                    with gr.Row():
                        pred_json = gr.JSON(label="é¢„æµ‹æ•°æ®", value={})
                        details_json = gr.JSON(label="æ¨ç†ç»†èŠ‚", value={})
                    features_markdown = gr.Markdown("### ç‰¹å¾æ‘˜è¦\n- æš‚æ— ç‰¹å¾ä¿¡æ¯ã€‚")
                    http_markdown = gr.Markdown("### ğŸŒ HTTP ä¿¡æ¯\n- ç­‰å¾…æ£€æµ‹")
                    cookie_markdown = gr.Markdown("### ğŸª Cookie ä¿¡æ¯\n- ç­‰å¾…æ£€æµ‹")
                    meta_markdown = gr.Markdown("### ğŸ§© Meta / æŒ‡çº¹ä¿¡æ¯\n- ç­‰å¾…æ£€æµ‹")
                    screenshot_image = gr.Image(label="é¡µé¢æˆªå›¾", visible=False)

                with gr.Row():
                    final_url = gr.Textbox(label="æœ€ç»ˆ URL", interactive=False)
                    status_code = gr.Textbox(label="çŠ¶æ€ç ", interactive=False)
                    content_type = gr.Textbox(label="å†…å®¹ç±»å‹", interactive=False)

                with gr.Accordion("ğŸ—‚ å†å²è®°å½•", open=False):
                    with gr.Row():
                        history_table = CompatDataFrame(
                            headers=["æ—¶é—´", "URL", "ç»¼åˆæ¦‚ç‡", "ç»“è®º"],
                            datatype=["str", "str", "str", "str"],
                            value=[],
                            interactive=False,
                        )
                        clear_history_btn = gr.Button("ğŸ§¹ æ¸…ç©ºè®°å½•", variant="secondary")

            with gr.TabItem("ğŸ“‹ æ‰¹é‡æ£€æµ‹"):
                gr.Markdown("### æ‰¹é‡æ£€æµ‹å¤šä¸ª URLï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰")
                urls_textarea = gr.TextArea(
                    label="è¾“å…¥ URL åˆ—è¡¨",
                    placeholder="https://example.com\nhttps://google.com",
                    lines=8,
                )
                with gr.Row():
                    batch_screenshot_cb = gr.Checkbox(label="å¯ç”¨æˆªå›¾", value=False)
                    batch_scan_btn = gr.Button("ğŸš€ å¼€å§‹æ‰¹é‡æ£€æµ‹", variant="primary")

                with gr.Accordion("ğŸ“ˆ æ‰¹é‡æ£€æµ‹ç»“æœ", open=True):
                    batch_status = gr.Markdown("ç­‰å¾…æ‰¹é‡æ£€æµ‹...")
                    results_table = CompatDataFrame(
                        headers=["URL", "æ£€æµ‹ç»“æœ", "é£é™©ç­‰çº§", "URL æ¨¡å‹", "FusionDNN", "ç»¼åˆæ¦‚ç‡", "å¤‡æ³¨"],
                        datatype=["str", "str", "str", "str", "str", "str", "str"],
                        value=[],
                        interactive=False,
                    )
                    results_file = gr.File(label="ä¸‹è½½ç»“æœ", visible=False)

            with gr.TabItem("ğŸ§ª æµ‹è¯•æ ·ä¾‹"):
                gr.Markdown("### é€‰æ‹©é¢„è®¾æ ·ä¾‹å¿«é€Ÿè¯„ä¼°")
                with gr.Row():
                    with gr.Column():
                        gr.Markdown("#### ğŸš¨ é’“é±¼ç½‘ç«™æ ·ä¾‹")
                        phishing_examples = CompatDataFrame(
                            value=[],
                            headers=["URL", "æè¿°"],
                            datatype=["str", "str"],
                            interactive=False,
                        )
                        load_phishing_btn = gr.Button("ğŸš¨ åŠ è½½é’“é±¼ç½‘ç«™æ ·ä¾‹", variant="stop")
                    with gr.Column():
                        gr.Markdown("#### âœ… è‰¯æ€§ç½‘ç«™æ ·ä¾‹")
                        benign_examples = CompatDataFrame(
                            value=[],
                            headers=["URL", "æè¿°"],
                            datatype=["str", "str"],
                            interactive=False,
                        )
                        load_benign_btn = gr.Button("âœ… åŠ è½½è‰¯æ€§ç½‘ç«™æ ·ä¾‹", variant="primary")
                        refresh_tables_btn = gr.Button("ğŸ”„ åˆ·æ–°è¡¨æ ¼æ•°æ®", variant="secondary")

                gr.Markdown("### ğŸ¯ å¿«é€Ÿæ£€æµ‹")
                with gr.Row():
                    test_url_input = gr.Textbox(
                        label="é€‰ä¸­åè‡ªåŠ¨å¡«å…¥ï¼ˆäº¦å¯æ‰‹å·¥è¾“å…¥ï¼‰",
                        value="https://www.baidu.com",
                    )
                    test_screenshot_cb = gr.Checkbox(label="å¯ç”¨æˆªå›¾åŠŸèƒ½", value=False)
                    test_scan_btn = gr.Button("ğŸ” å¼€å§‹æ£€æµ‹", variant="primary")

                with gr.Row():
                    test_conclusion = gr.Markdown("è¯·é€‰æ‹© URL å¹¶ç‚¹å‡»æ£€æµ‹")
                    test_status = gr.HTML("<div style='text-align:center;padding:20px;'>â³ ç­‰å¾…æ£€æµ‹...</div>")

            with gr.TabItem("â„¹ï¸ ç³»ç»Ÿä¿¡æ¯"):
                gr.Markdown(
                    """
                    ### ğŸ¤– æ¨¡å‹ä¿¡æ¯
                    - **URL é¢„è®­ç»ƒæ¨¡å‹**ï¼š`imanoop7/bert-phishing-detector`
                    - **FusionDNN æ¨¡å‹**ï¼š42 ç‰¹å¾å¢å¼ºç‰ˆæœ¬

                    ### âš™ï¸ æ¨ç†é…ç½®
                    - URL é˜ˆå€¼ï¼š`settings.url_phish_threshold`
                    - Fusion é˜ˆå€¼ï¼š`settings.fusion_phish_threshold`
                    - æœ€ç»ˆé˜ˆå€¼ï¼š`settings.final_phish_threshold`

                    ### ğŸ“ˆ æ¨èæµç¨‹
                    1. è¾“å…¥æˆ–æ‰¹é‡ç²˜è´´ URL
                    2. æŸ¥çœ‹æ¦‚ç‡æ‹†è§£ & æ¨ç†ç»†èŠ‚
                    3. ä¸‹è½½æ‰¹é‡ CSV å¤æ ¸/ç•™æ¡£
                    """
                )

        def on_scan_click(url: str, screenshot: bool, history: List[Dict[str, Any]]):
            # è¾“å…¥éªŒè¯
            if not url or not url.strip():
                error_result = ValueError("è¯·è¾“å…¥è¦æ£€æµ‹çš„URL")
                return update_single_result(error_result, history)

            try:
                result = asyncio.run(scan_single(url, screenshot))
                return update_single_result(result, history)
            except ValueError as ve:
                # å¤„ç†URLæ ¼å¼é”™è¯¯
                return update_single_result(ve, history)
            except Exception as exc:
                return update_single_result(exc, history)

        scan_btn.click(
            fn=on_scan_click,
            inputs=[url_input, screenshot_cb, history_state],
            outputs=[
                conclusion_box,
                status_indicator,
                probability_summary,
                detail_summary,
                pred_json,
                details_json,
                features_markdown,
                http_markdown,
                cookie_markdown,
                meta_markdown,
                screenshot_image,
                final_url,
                status_code,
                content_type,
                history_table,
                history_state,
            ],
        )

        def on_test_scan(url: str, screenshot: bool, history: List[Dict[str, Any]]):
            # è¾“å…¥éªŒè¯
            if not url or not url.strip():
                result = ValueError("è¯·è¾“å…¥è¦æ£€æµ‹çš„URL")
            else:
                try:
                    result = asyncio.run(scan_single(url, screenshot))
                except ValueError as ve:
                    result = ve
                except Exception as exc:
                    result = exc
            (
                conclusion,
                status_html,
                prob_summary,
                detail_summary_val,
                pred_data,
                pred_details,
                features_text,
                http_summary,
                cookie_summary,
                meta_summary,
                screenshot_update,
                final_url_val,
                status_code_val,
                content_type_val,
                history_table_update,
                history_value,
            ) = update_single_result(result, history)
            return (
                conclusion,
                status_html,
                conclusion,
                status_html,
                prob_summary,
                detail_summary_val,
                pred_data,
                pred_details,
                features_text,
                http_summary,
                cookie_summary,
                meta_summary,
                screenshot_update,
                final_url_val,
                status_code_val,
                content_type_val,
                history_table_update,
                history_value,
            )

        test_scan_btn.click(
            fn=on_test_scan,
            inputs=[test_url_input, test_screenshot_cb, history_state],
            outputs=[
                test_conclusion,
                test_status,
                conclusion_box,
                status_indicator,
                probability_summary,
                detail_summary,
                pred_json,
                details_json,
                features_markdown,
                http_markdown,
                cookie_markdown,
                meta_markdown,
                screenshot_image,
                final_url,
                status_code,
                content_type,
                history_table,
                history_state,
            ],
        )

        def clear_history():
            return gr.update(value=[]), []

        clear_history_btn.click(fn=clear_history, outputs=[history_table, history_state])

        def on_batch_scan(urls: str, screenshot: bool):
            # è¾“å…¥éªŒè¯
            if not urls or not urls.strip():
                return (
                    "âŒ è¯·è¾“å…¥è¦æ£€æµ‹çš„URLåˆ—è¡¨",
                    gr.update(value=[]),
                    gr.update(value=None, visible=False),
                )

            # éªŒè¯URLæ ¼å¼
            url_lines = [line.strip() for line in urls.splitlines() if line.strip()]
            if not url_lines:
                return (
                    "âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„URL",
                    gr.update(value=[]),
                    gr.update(value=None, visible=False),
                )

            try:
                results = asyncio.run(scan_multiple(urls, screenshot))
                rows, csv_path, stats = build_batch_results(results)
                summary = (
                    f"âœ… å…±æ£€æµ‹ {stats['total']} æ¡ URLï¼Œå…¶ä¸­é’“é±¼ {stats['phish']} æ¡"
                    + (f"ï¼Œå¤±è´¥ {stats['errors']} æ¡" if stats['errors'] else "")
                )
                return (
                    summary,
                    gr.update(value=rows),
                    gr.update(value=csv_path, visible=True),
                )
            except ValueError as ve:
                # å¤„ç†URLæ ¼å¼é”™è¯¯
                return (
                    f"âŒ URLæ ¼å¼é”™è¯¯ï¼š{ve}",
                    gr.update(value=[]),
                    gr.update(value=None, visible=False),
                )
            except Exception as exc:
                return (
                    f"âŒ æ‰¹é‡æ£€æµ‹å¤±è´¥ï¼š{exc}",
                    gr.update(value=[]),
                    gr.update(value=None, visible=False),
                )

        batch_scan_btn.click(
            fn=on_batch_scan,
            inputs=[urls_textarea, batch_screenshot_cb],
            outputs=[batch_status, results_table, results_file],
        )

        def load_phishing_examples():
            sample_data = Path("data_massive/dataset_batch1_final.parquet")
            if sample_data.exists():
                df = pd.read_parquet(sample_data)
                urls = df[df["label"] == 1]["final_url"].dropna().head(50).tolist()
                first_url = urls[0] if urls else ""
                return "\n".join(urls), first_url, first_url
            else:
                fallback = [
                    "http://verify-paypal-account.com",
                    "http://apple-security-update.info",
                    "http://microsoft-login-alert.com",
                    "http://amazon-gift-card-winner.com",
                    "http://bank-of-america-verify.com",
                ]
                return "\n".join(fallback), fallback[0], fallback[0]

        def load_benign_examples():
            sample_data = Path("data_massive/dataset_batch1_final.parquet")
            if sample_data.exists():
                df = pd.read_parquet(sample_data)
                urls = df[df["label"] == 0]["final_url"].dropna().head(50).tolist()
                first_url = urls[0] if urls else ""
                return "\n".join(urls), first_url, first_url
            else:
                fallback = [
                    "https://www.baidu.com",
                    "https://www.google.com",
                    "https://github.com",
                    "https://www.wikipedia.org",
                    "https://www.taobao.com",
                ]
                return "\n".join(fallback), fallback[0], fallback[0]

        def update_tables_from_data():
            sample_data = Path("data_massive/dataset_batch1_final.parquet")
            if not sample_data.exists():
                return gr.update(), gr.update()

            df = pd.read_parquet(sample_data)

            phishing_data = df[df["label"] == 1][["final_url", "html"]].head(50)
            phishing_rows = []
            for _, row in phishing_data.iterrows():
                url = row["final_url"] or ""
                desc = "é’“é±¼ç½‘ç«™" + (f" - {len(row['html'])}å­—ç¬¦" if pd.notna(row['html']) and len(str(row['html'])) > 0 else "")
                phishing_rows.append([url, desc])

            benign_data = df[df["label"] == 0][["final_url", "html"]].head(50)
            benign_rows = []
            for _, row in benign_data.iterrows():
                url = row["final_url"] or ""
                desc = "è‰¯æ€§ç½‘ç«™" + (f" - {len(row['html'])}å­—ç¬¦" if pd.notna(row['html']) and len(str(row['html'])) > 0 else "")
                benign_rows.append([url, desc])

            return gr.update(value=phishing_rows), gr.update(value=benign_rows)

        def on_select_phish(evt: gr.SelectData):
            return evt.value, evt.value

        def on_select_benign(evt: gr.SelectData):
            return evt.value, evt.value

        load_phishing_btn.click(
            fn=load_phishing_examples,
            outputs=[urls_textarea, url_input, test_url_input]
        )
        load_benign_btn.click(
            fn=load_benign_examples,
            outputs=[urls_textarea, url_input, test_url_input]
        )

        phishing_examples.select(
            fn=on_select_phish,
            outputs=[test_url_input, url_input]
        )
        benign_examples.select(
            fn=on_select_benign,
            outputs=[test_url_input, url_input]
        )

        refresh_tables_btn.click(
            fn=update_tables_from_data,
            outputs=[phishing_examples, benign_examples]
        )

    return demo


def main():
    parser = argparse.ArgumentParser(description="Launch PhishGuard UI")
    parser.add_argument("--host", type=str, default=None, help="è‡ªå®šä¹‰ç›‘å¬åœ°å€")
    parser.add_argument("--port", type=int, default=None, help="è‡ªå®šä¹‰ç›‘å¬ç«¯å£ (0 è¡¨ç¤ºè‡ªåŠ¨åˆ†é…)")
    args = parser.parse_args()

    server_name = args.host if args.host is not None else os.environ.get("GRADIO_SERVER_NAME", "0.0.0.0")
    if args.port is not None:
        server_port = None if args.port == 0 else args.port
    else:
        env_port = os.environ.get("GRADIO_SERVER_PORT")
        server_port = None if env_port in {None, "", "0"} else int(env_port)

    share_default = os.environ.get("PHISHGUARD_SHARE", "true").lower() != "false"

    demo = build_interface()
    demo.launch(
        server_name=server_name,
        server_port=server_port,
        share=share_default,
        show_api=False,
    )


if __name__ == "__main__":
    main()
