#!/usr/bin/env python3
"""
ä»GitHubæ”¶é›†é«˜è´¨é‡é’“é±¼ç½‘ç«™æ•°æ®é›†
"""

import requests
import json
import time
import re
from typing import List, Dict, Any
from pathlib import Path
import pandas as pd

class GitHubPhishCollector:
    """GitHubé’“é±¼ç½‘ç«™æ•°æ®æ”¶é›†å™¨"""

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.collected_data = []

    def search_github_repositories(self, keywords: List[str]) -> List[Dict]:
        """æœç´¢GitHubä»“åº“"""
        repositories = []

        for keyword in keywords:
            print(f"ğŸ” æœç´¢å…³é”®è¯: {keyword}")

            # GitHub APIæœç´¢
            search_url = f"https://api.github.com/search/repositories?q={keyword}+phishing&sort=stars&order=desc"
            try:
                response = self.session.get(search_url)
                if response.status_code == 200:
                    data = response.json()
                    for repo in data.get('items', [])[:5]:  # å–å‰5ä¸ª
                        repositories.append({
                            'name': repo['full_name'],
                            'url': repo['html_url'],
                            'stars': repo['stargazers_count'],
                            'description': repo['description'],
                            'language': repo.get('language', 'Unknown')
                        })
                        print(f"  ğŸ“¦ å‘ç°ä»“åº“: {repo['full_name']} (â­{repo['stargazers_count']})")
            except Exception as e:
                print(f"  âŒ æœç´¢å¤±è´¥: {e}")

            time.sleep(1)  # é¿å…APIé™åˆ¶

        return repositories

    def extract_urls_from_github_files(self, repo_url: str) -> List[str]:
        """ä»GitHubä»“åº“æ–‡ä»¶ä¸­æå–URL"""
        urls = []

        try:
            # è½¬æ¢ä¸ºrawå†…å®¹URL
            raw_url = repo_url.replace('github.com', 'raw.githubusercontent.com').replace('/tree/', '/')

            # å¸¸è§çš„é’“é±¼ç½‘ç«™æ•°æ®æ–‡ä»¶è·¯å¾„
            file_paths = [
                'phishing_urls.txt',
                'urls.txt',
                'malicious_urls.txt',
                'suspicious_domains.txt',
                'phishing_domains.txt',
                'data/phishing.txt',
                'datasets/urls.txt'
            ]

            for file_path in file_paths:
                file_url = f"{raw_url}/{file_path}"
                try:
                    response = self.session.get(file_url, timeout=10)
                    if response.status_code == 200:
                        content = response.text
                        # æå–URL
                        extracted = self.extract_urls_from_text(content)
                        urls.extend(extracted)
                        print(f"  ğŸ“„ ä» {file_path} æå–äº† {len(extracted)} ä¸ªURL")
                        break
                except:
                    continue

        except Exception as e:
            print(f"  âŒ å¤„ç†ä»“åº“å¤±è´¥: {e}")

        return urls

    def extract_urls_from_text(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–URL"""
        url_patterns = [
            r'https?://[^\s<>"{}|\\^`\[\]]+',
            r'http?://[^\s<>"{}|\\^`\[\]]+',
            r'[a-zA-Z0-9][a-zA-Z0-9-]{1,61}[a-zA-Z0-9]\.[a-zA-Z]{2,}',
            r'www\.[a-zA-Z0-9-]+\.[a-zA-Z]{2,}'
        ]

        urls = []
        for pattern in url_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                # æ¸…ç†å’Œæ ‡å‡†åŒ–URL
                url = match.strip().rstrip('.,;:!?\'"()[]{}')
                if not url.startswith(('http://', 'https://')):
                    url = 'http://' + url
                urls.append(url)

        return list(set(urls))  # å»é‡

    def collect_from_known_sources(self) -> List[Dict[str, Any]]:
        """ä»å·²çŸ¥çš„é’“é±¼ç½‘ç«™æ•°æ®æºæ”¶é›†"""
        known_sources = [
            {
                'name': 'PhishTank',
                'url': 'https://raw.githubusercontent.com/mitchellkrogza/Phishing.Database/master/phishing-links-ACTIVE.txt',
                'type': 'active_phishing'
            },
            {
                'name': 'OpenPhish',
                'url': 'https://raw.githubusercontent.com/mitchellkrogza/Phishing.Database/master/phishing-links-OPENPHISH.txt',
                'type': 'openphish'
            },
            {
                'name': 'URLHaus',
                'url': 'https://raw.githubusercontent.com/mitchellkrogza/Phishing.Database/master/phishing-links-URLHAUS.txt',
                'type': 'urlhaus'
            },
            {
                'name': 'Certego',
                'url': 'https://raw.githubusercontent.com/certego/chromium-phishing-domains/main/chromium-phishing-domains.txt',
                'type': 'certego'
            }
        ]

        collected_data = []

        for source in known_sources:
            print(f"ğŸ¯ æ”¶é›†æ¥æº: {source['name']}")

            try:
                response = self.session.get(source['url'], timeout=30)
                if response.status_code == 200:
                    content = response.text
                    urls = self.extract_urls_from_text(content)

                    for url in urls[:100]:  # æ¯ä¸ªæ¥æºæœ€å¤š100ä¸ª
                        collected_data.append({
                            'url': url,
                            'source': source['name'],
                            'type': source['type'],
                            'label': 1,  # é’“é±¼ç½‘ç«™
                            'confidence': 'high',
                            'collected_at': time.time()
                        })

                    print(f"  âœ… æ”¶é›†äº† {min(len(urls), 100)} ä¸ªURL")
                else:
                    print(f"  âŒ è¯·æ±‚å¤±è´¥: {response.status_code}")

            except Exception as e:
                print(f"  âŒ æ”¶é›†å¤±è´¥: {e}")

            time.sleep(2)  # é¿å…è¯·æ±‚è¿‡å¿«

        return collected_data

    def collect_high_quality_benign(self) -> List[Dict[str, Any]]:
        """æ”¶é›†é«˜è´¨é‡è‰¯æ€§ç½‘ç«™"""
        benign_sources = {
            'top_companies': [
                'https://www.google.com',
                'https://www.microsoft.com',
                'https://www.apple.com',
                'https://www.amazon.com',
                'https://www.facebook.com',
                'https://www.twitter.com',
                'https://www.linkedin.com',
                'https://www.github.com',
                'https://www.youtube.com',
                'https://www.netflix.com',
                'https://www.spotify.com',
                'https://www.adobe.com',
                'https://www.oracle.com',
                'https://www.ibm.com',
                'https://www.intel.com'
            ],
            'financial': [
                'https://www.bankofamerica.com',
                'https://www.chase.com',
                'https://www.wellsfargo.com',
                'https://www.citibank.com',
                'https://www.hsbc.com',
                'https://www.barclays.com',
                'https://www.goldmansachs.com',
                'https://www.morganstanley.com'
            ],
            'government': [
                'https://www.gov.cn',
                'https://www.whitehouse.gov',
                'https://www.europa.eu',
                'https://www.gov.uk',
                'https://www.canada.ca',
                'https://www.australia.gov.au'
            ],
            'education': [
                'https://www.harvard.edu',
                'https://www.mit.edu',
                'https://www.stanford.edu',
                'https://www.berkeley.edu',
                'https://www.cmu.edu',
                'https://www.tsinghua.edu.cn',
                'https://www.pku.edu.cn'
            ]
        }

        benign_data = []

        for category, urls in benign_sources.items():
            for url in urls:
                benign_data.append({
                    'url': url,
                    'source': category,
                    'type': 'benign',
                    'label': 0,  # è‰¯æ€§ç½‘ç«™
                    'confidence': 'high',
                    'collected_at': time.time()
                })

        return benign_data

    def validate_urls(self, urls: List[str]) -> List[str]:
        """éªŒè¯URLæ˜¯å¦å¯è®¿é—®"""
        valid_urls = []

        for url in urls[:50]:  # é™åˆ¶éªŒè¯æ•°é‡
            try:
                response = self.session.head(url, timeout=10, allow_redirects=True)
                if response.status_code in [200, 301, 302, 303, 307, 308]:
                    valid_urls.append(url)
                    print(f"  âœ… {url}")
                else:
                    print(f"  âš ï¸  {url} - {response.status_code}")
            except:
                print(f"  âŒ {url} - ä¸å¯è®¿é—®")

            time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«

        return valid_urls

    def run_collection(self):
        """è¿è¡Œå®Œæ•´çš„æ•°æ®æ”¶é›†æµç¨‹"""
        print("ğŸš€ å¼€å§‹GitHubé«˜è´¨é‡é’“é±¼ç½‘ç«™æ•°æ®æ”¶é›†")
        print("=" * 60)

        all_data = []

        # 1. ä»å·²çŸ¥æ¥æºæ”¶é›†é’“é±¼ç½‘ç«™
        print("\nğŸ¯ æ­¥éª¤1: æ”¶é›†é’“é±¼ç½‘ç«™æ•°æ®")
        phishing_data = self.collect_from_known_sources()
        all_data.extend(phishing_data)

        # 2. æ”¶é›†è‰¯æ€§ç½‘ç«™
        print(f"\nâœ… æ­¥éª¤2: æ”¶é›†è‰¯æ€§ç½‘ç«™æ•°æ®")
        benign_data = self.collect_high_quality_benign()
        all_data.extend(benign_data)

        # 3. æœç´¢GitHubä»“åº“
        print(f"\nğŸ” æ­¥éª¤3: æœç´¢GitHubä»“åº“")
        keywords = [
            'phishing dataset',
            'malicious URLs',
            'phishing domains',
            'security dataset',
            'threat intelligence'
        ]

        repositories = self.search_github_repositories(keywords)

        # 4. ä»GitHubä»“åº“æå–URL
        print(f"\nğŸ“¦ æ­¥éª¤4: ä»GitHubä»“åº“æå–URL")
        github_urls = []
        for repo in repositories[:3]:  # å¤„ç†å‰3ä¸ªä»“åº“
            print(f"\nå¤„ç†ä»“åº“: {repo['name']}")
            urls = self.extract_urls_from_github_files(repo['url'])
            github_urls.extend(urls)

        # æ·»åŠ GitHubæ”¶é›†çš„é’“é±¼ç½‘ç«™
        for url in github_urls[:200]:  # é™åˆ¶æ•°é‡
            all_data.append({
                'url': url,
                'source': 'github_repos',
                'type': 'github_phishing',
                'label': 1,
                'confidence': 'medium',
                'collected_at': time.time()
            })

        # 5. ä¿å­˜æ•°æ®
        print(f"\nğŸ’¾ æ­¥éª¤5: ä¿å­˜æ•°æ®")
        df = pd.DataFrame(all_data)

        # æ•°æ®ç»Ÿè®¡
        phishing_count = len(df[df['label'] == 1])
        benign_count = len(df[df['label'] == 0])

        print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
        print(f"  æ€»è®°å½•æ•°: {len(df)}")
        print(f"  é’“é±¼ç½‘ç«™: {phishing_count}")
        print(f"  è‰¯æ€§ç½‘ç«™: {benign_count}")

        # ä¿å­˜æ•°æ®
        output_dir = Path("github_data")
        output_dir.mkdir(exist_ok=True)

        # ä¿å­˜åŸå§‹æ•°æ®
        df.to_parquet(output_dir / "raw_github_data.parquet", index=False)

        # ä¿å­˜åˆ†ç±»æ•°æ®
        phishing_df = df[df['label'] == 1]
        benign_df = df[df['label'] == 0]

        phishing_df.to_parquet(output_dir / "phishing_urls.parquet", index=False)
        benign_df.to_parquet(output_dir / "benign_urls.parquet", index=False)

        # ä¿å­˜URLåˆ—è¡¨
        with open(output_dir / "phishing_urls.txt", "w", encoding="utf-8") as f:
            for url in phishing_df['url'].unique():
                f.write(url + "\n")

        with open(output_dir / "benign_urls.txt", "w", encoding="utf-8") as f:
            for url in benign_df['url'].unique():
                f.write(url + "\n")

        print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ° {output_dir}/")
        print(f"  - raw_github_data.parquet: å®Œæ•´æ•°æ®é›†")
        print(f"  - phishing_urls.parquet: é’“é±¼ç½‘ç«™æ•°æ®")
        print(f"  - benign_urls.parquet: è‰¯æ€§ç½‘ç«™æ•°æ®")
        print(f"  - phishing_urls.txt: é’“é±¼ç½‘ç«™URLåˆ—è¡¨")
        print(f"  - benign_urls.txt: è‰¯æ€§ç½‘ç«™URLåˆ—è¡¨")

        return df

if __name__ == "__main__":
    collector = GitHubPhishCollector()
    data = collector.run_collection()