#!/usr/bin/env python3
"""
åŸºäºçœŸå®é’“é±¼ç½‘ç«™æ„å»ºè®­ç»ƒæ•°æ®é›†
"""

import asyncio
import httpx
import pandas as pd
import numpy as np
from pathlib import Path
from phishguard_v1.features.fetcher import fetch_one
from phishguard_v1.features.parser import extract_from_html
from phishguard_v1.features.url_features import url_stats
from phishguard_v1.features.feature_engineering import compute_feature_dict, FEATURE_COLUMNS

async def collect_phishing_data():
    """æ”¶é›†é’“é±¼ç½‘ç«™æ•°æ®"""
    print("ğŸ” æ”¶é›†çœŸå®é’“é±¼ç½‘ç«™è®­ç»ƒæ•°æ®...")

    # çœŸå®é’“é±¼ç½‘ç«™åˆ—è¡¨
    phishing_urls = [
        "http://wells-fargo-login.com",
        "http://citibank-online.com",
        "http://paypal-verification.net",
        "http://www.paypal-verification.net",
        "http://paypal-verification.org",
        "http://apple-id-verify.com",
        "http://login.apple.com.id-verify.com",
        "http://gmail-security-alert.com",
        "http://facebook.login-secure.net",
        "http://www.facebook.login-secure.net",
        "http://snapchat-verify.com",
        "https://account-verification.net",
        "http://secure.paypal.com.verify-account.com",
        "http://paypa1.com",
        "http://arnazon.com",
        "http://faceb00k.com",
    ]

    # åˆæ³•ç½‘ç«™
    legitimate_urls = [
        "https://www.google.com",
        "https://www.github.com",
        "https://www.microsoft.com",
        "https://www.apple.com",
        "https://www.wikipedia.org",
        "https://www.stackoverflow.com",
        "https://www.paypal.com",
        "https://www.facebook.com",
        "https://www.linkedin.com",
        "https://www.amazon.com",
        "https://www.instagram.com",
        "https://www.twitter.com",
        "https://www.netflix.com",
        "https://www.youtube.com",
        "https://www.reddit.com",
    ]

    print(f"ğŸ“Š ç›®æ ‡: {len(phishing_urls)} é’“é±¼ + {len(legitimate_urls)} åˆæ³•")

    async with httpx.AsyncClient(timeout=15.0) as client:
        phishing_data = []
        legitimate_data = []

        # æ”¶é›†é’“é±¼ç½‘ç«™æ•°æ®
        print(f"\nğŸ£ æ”¶é›†é’“é±¼ç½‘ç«™æ•°æ®...")
        for i, url in enumerate(phishing_urls):
            print(f"  {i+1}/{len(phishing_urls)}: {url}")
            try:
                item = await fetch_one(url.strip(), client)
                html_feats = extract_from_html(
                    item.get("html", ""),
                    item.get("final_url") or item.get("request_url")
                )
                item["html_feats"] = html_feats
                item["label"] = 1
                phishing_data.append(item)
                print(f"    âœ… æˆåŠŸ")
            except Exception as e:
                print(f"    âŒ å¤±è´¥: {e}")

        # æ”¶é›†åˆæ³•ç½‘ç«™æ•°æ®
        print(f"\nâœ… æ”¶é›†åˆæ³•ç½‘ç«™æ•°æ®...")
        for i, url in enumerate(legitimate_urls):
            print(f"  {i+1}/{len(legitimate_urls)}: {url}")
            try:
                item = await fetch_one(url.strip(), client)
                html_feats = extract_from_html(
                    item.get("html", ""),
                    item.get("final_url") or item.get("request_url")
                )
                item["html_feats"] = html_feats
                item["label"] = 0
                legitimate_data.append(item)
                print(f"    âœ… æˆåŠŸ")
            except Exception as e:
                print(f"    âŒ å¤±è´¥: {e}")

        print(f"\nğŸ“Š æ•°æ®æ”¶é›†å®Œæˆ:")
        print(f"  é’“é±¼ç½‘ç«™: {len(phishing_data)} ä¸ª")
        print(f"  åˆæ³•ç½‘ç«™: {len(legitimate_data)} ä¸ª")

        return phishing_data, legitimate_data

def create_training_dataset(phishing_data, legitimate_data):
    """åˆ›å»ºè®­ç»ƒæ•°æ®é›†"""
    print(f"\nğŸ”§ åˆ›å»ºè®­ç»ƒæ•°æ®é›†...")

    # åˆå¹¶æ•°æ®
    all_data = phishing_data + legitimate_data

    # æå–ç‰¹å¾
    features_list = []
    labels_list = []

    for item in all_data:
        try:
            # è®¡ç®—å®Œæ•´ç‰¹å¾
            feature_dict = compute_feature_dict(item)

            # ç¡®ä¿æ‰€æœ‰ç‰¹å¾éƒ½å­˜åœ¨
            feature_vector = [feature_dict.get(col, 0.0) for col in FEATURE_COLUMNS]

            features_list.append(feature_vector)
            labels_list.append(item["label"])
        except Exception as e:
            print(f"âŒ ç‰¹å¾æå–å¤±è´¥: {e}")
            continue

    # è½¬æ¢ä¸ºDataFrame
    feature_df = pd.DataFrame(features_list, columns=FEATURE_COLUMNS)
    label_df = pd.DataFrame({"label": labels_list})

    print(f"âœ… ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {feature_df.shape}")
    print(f"ğŸ“Š æ ‡ç­¾åˆ†å¸ƒ: {label_df['label'].value_counts().to_dict()}")

    return feature_df, label_df

def augment_dataset(features_df, labels_df, target_size=1000):
    """æ•°æ®å¢å¼ºä»¥å¢åŠ è®­ç»ƒæ•°æ®"""
    print(f"\nğŸ”„ æ•°æ®å¢å¼º (ç›®æ ‡å¤§å°: {target_size})...")

    current_size = len(features_df)
    if current_size >= target_size:
        print(f"âœ… æ•°æ®é‡å·²è¶³å¤Ÿ: {current_size}")
        return features_df, labels_df

    # è®¡ç®—éœ€è¦å¢åŠ çš„æ ·æœ¬æ•°
    samples_needed = target_size - current_size
    print(f"ğŸ“ˆ éœ€è¦å¢åŠ  {samples_needed} ä¸ªæ ·æœ¬")

    augmented_features = []
    augmented_labels = []

    while len(augmented_features) < samples_needed:
        # éšæœºé€‰æ‹©ä¸€ä¸ªç°æœ‰æ ·æœ¬è¿›è¡Œå¢å¼º
        idx = np.random.randint(0, current_size)
        original_features = features_df.iloc[idx].values
        original_label = labels_df.iloc[idx]['label']

        # åˆ›å»ºå¢å¼ºæ ·æœ¬
        noise_level = 0.1 if original_label == 0 else 0.15  # é’“é±¼ç½‘ç«™å¢åŠ æ›´å¤šå™ªå£°

        # æ·»åŠ é«˜æ–¯å™ªå£°
        noise = np.random.normal(0, noise_level, original_features.shape)
        noisy_features = original_features + noise

        # ç¡®ä¿ç‰¹å¾å€¼åˆç†
        noisy_features = np.maximum(noisy_features, 0)  # éè´Ÿå€¼

        # é™åˆ¶æŸäº›ç‰¹å¾çš„åˆç†èŒƒå›´
        for i, col in enumerate(FEATURE_COLUMNS):
            if col.endswith("_len"):  # é•¿åº¦ç‰¹å¾
                noisy_features[i] = min(noisy_features[i], 1000)
            elif col.endswith("_ratio"):  # æ¯”ç‡ç‰¹å¾
                noisy_features[i] = np.clip(noisy_features[i], 0, 1)
            elif col in ["status_code"]:  # çŠ¶æ€ç 
                noisy_features[i] = np.clip(noisy_features[i], 100, 600)

        augmented_features.append(noisy_features)
        augmented_labels.append(original_label)

    # åˆ›å»ºå¢å¼ºæ•°æ®
    augmented_features_df = pd.DataFrame(augmented_features, columns=FEATURE_COLUMNS)
    augmented_labels_df = pd.DataFrame({"label": augmented_labels})

    # åˆå¹¶åŸå§‹æ•°æ®å’Œå¢å¼ºæ•°æ®
    final_features = pd.concat([features_df, augmented_features_df], ignore_index=True)
    final_labels = pd.concat([labels_df, augmented_labels_df], ignore_index=True)

    print(f"âœ… å¢å¼ºåæ•°æ®é›†å¤§å°: {len(final_features)}")
    print(f"ğŸ“Š æœ€ç»ˆæ ‡ç­¾åˆ†å¸ƒ: {final_labels['label'].value_counts().to_dict()}")

    return final_features, final_labels

def save_dataset(features_df, labels_df, name="real_phishing_dataset"):
    """ä¿å­˜æ•°æ®é›†"""
    print(f"\nğŸ’¾ ä¿å­˜æ•°æ®é›†...")

    # åˆå¹¶ç‰¹å¾å’Œæ ‡ç­¾
    dataset_df = pd.concat([features_df, labels_df], axis=1)

    # ä¿å­˜å®Œæ•´æ•°æ®é›†
    dataset_path = Path(f"data_{name}/dataset.parquet")
    dataset_path.parent.mkdir(exist_ok=True)
    dataset_df.to_parquet(dataset_path, index=False)
    print(f"âœ… å®Œæ•´æ•°æ®é›†å·²ä¿å­˜: {dataset_path}")

    # åˆ†å‰²å¹¶ä¿å­˜è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†
    # æ‰“ä¹±æ•°æ®
    dataset_df = dataset_df.sample(frac=1, random_state=42).reset_index(drop=True)

    # åˆ†å‰²
    total_size = len(dataset_df)
    train_size = int(total_size * 0.7)
    val_size = int(total_size * 0.15)

    train_df = dataset_df[:train_size]
    val_df = dataset_df[train_size:train_size + val_size]
    test_df = dataset_df[train_size + val_size:]

    # ä¿å­˜åˆ†å‰²åçš„æ•°æ®
    train_path = dataset_path.parent / "train.parquet"
    val_path = dataset_path.parent / "val.parquet"
    test_path = dataset_path.parent / "test.parquet"

    train_df.to_parquet(train_path, index=False)
    val_df.to_parquet(val_path, index=False)
    test_df.to_parquet(test_path, index=False)

    print(f"âœ… è®­ç»ƒé›†: {train_path} ({len(train_df)} æ ·æœ¬)")
    print(f"âœ… éªŒè¯é›†: {val_path} ({len(val_df)} æ ·æœ¬)")
    print(f"âœ… æµ‹è¯•é›†: {test_path} ({len(test_df)} æ ·æœ¬)")

    return dataset_path.parent

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ„å»ºåŸºäºçœŸå®é’“é±¼ç½‘ç«™çš„è®­ç»ƒæ•°æ®é›†")
    print("=" * 60)

    # æ”¶é›†æ•°æ®
    phishing_data, legitimate_data = await collect_phishing_data()

    if len(phishing_data) == 0 or len(legitimate_data) == 0:
        print("âŒ æ•°æ®æ”¶é›†å¤±è´¥ï¼Œæ— æ³•æ„å»ºæ•°æ®é›†")
        return None

    # åˆ›å»ºæ•°æ®é›†
    features_df, labels_df = create_training_dataset(phishing_data, legitimate_data)

    # æ•°æ®å¢å¼º
    augmented_features, augmented_labels = augment_dataset(features_df, labels_df, target_size=1000)

    # ä¿å­˜æ•°æ®é›†
    dataset_dir = save_dataset(augmented_features, augmented_labels, "real_phishing_v2")

    print(f"\nğŸ‰ çœŸå®é’“é±¼ç½‘ç«™æ•°æ®é›†æ„å»ºå®Œæˆ!")
    print(f"ğŸ“ æ•°æ®é›†ç›®å½•: {dataset_dir}")
    print(f"ğŸ”¥ ç°åœ¨å¯ä»¥è®­ç»ƒæ›´å¼ºæ¨¡å‹äº†!")

    return dataset_dir

if __name__ == "__main__":
    asyncio.run(main())