#!/usr/bin/env python3
"""
éªŒè¯GitHubæ”¶é›†çš„URLå¹¶æå–ç‰¹å¾ï¼Œæ„å»ºé«˜è´¨é‡è®­ç»ƒæ•°æ®é›†
"""

import asyncio
import httpx
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any
import json
import time
import sys
sys.path.append('.')

from phishguard_v1.features.fetcher import fetch_one
from phishguard_v1.config import settings

async def validate_and_extract_features(urls: List[str], label: int, batch_size: int = 10) -> List[Dict[str, Any]]:
    """éªŒè¯URLå¹¶æå–ç‰¹å¾"""
    results = []

    # é…ç½®ä¼šè¯
    timeout = httpx.Timeout(30.0)
    headers = {"User-Agent": settings.user_agent}

    async with httpx.AsyncClient(timeout=timeout, headers=headers) as client:
        for i in range(0, len(urls), batch_size):
            batch = urls[i:i + batch_size]
            print(f"å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(urls)-1)//batch_size + 1} ({len(batch)} ä¸ªURL)")

            tasks = []
            for url in batch:
                task = process_single_url(client, url, label)
                tasks.append(task)

            batch_results = await asyncio.gather(*tasks, return_exceptions=True)

            for result in batch_results:
                if isinstance(result, Exception):
                    print(f"  âŒ å¤„ç†å¤±è´¥: {result}")
                elif result:
                    results.append(result)
                    status = "âœ…" if result['success'] else "âŒ"
                    print(f"  {status} {result['url']} - çŠ¶æ€: {result.get('status_code', 'N/A')}")

            # æ‰¹æ¬¡é—´å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
            if i + batch_size < len(urls):
                await asyncio.sleep(2)

    return results

async def process_single_url(client: httpx.AsyncClient, url: str, label: int) -> Dict[str, Any]:
    """å¤„ç†å•ä¸ªURL"""
    try:
        # ä½¿ç”¨æˆ‘ä»¬çš„fetcherè·å–ç½‘é¡µå†…å®¹
        item = await fetch_one(url, client)

        if not item or not item.get('ok', False):
            return {
                'url': url,
                'label': label,
                'success': False,
                'error': item.get('meta', {}).get('error', 'æŠ“å–å¤±è´¥') if item else 'æŠ“å–å¤±è´¥',
                'status_code': item.get('status_code') if item else None
            }

        # ç‰¹å¾å·²ç»åœ¨fetch_oneä¸­æå–å®Œæˆ
        url_feats = item.get('url_feats', {})
        html_feats = item.get('html_feats', {})

        # åˆå¹¶ç‰¹å¾
        features = {}
        features.update(url_feats)
        features.update(html_feats)
        features.update({
            'status_code': item.get('status_code'),
            'content_type': item.get('content_type'),
            'bytes': item.get('bytes', 0)
        })

        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        result = {
            'url': url,
            'final_url': item.get('final_url', url),
            'label': label,
            'success': True,
            'status_code': item.get('status_code'),
            'content_type': item.get('content_type'),
            'bytes': item.get('bytes', 0),
            'features': features,
            'timestamp': time.time()
        }

        return result

    except Exception as e:
        return {
            'url': url,
            'label': label,
            'success': False,
            'error': str(e),
            'timestamp': time.time()
        }

def load_github_data() -> tuple[List[str], List[str]]:
    """åŠ è½½GitHubæ”¶é›†çš„æ•°æ®"""
    github_dir = Path("github_data")

    # è¯»å–é’“é±¼ç½‘ç«™URL
    phishing_urls = []
    if (github_dir / "phishing_urls.txt").exists():
        with open(github_dir / "phishing_urls.txt", "r", encoding="utf-8") as f:
            phishing_urls = [line.strip() for line in f if line.strip()]

    # è¯»å–è‰¯æ€§ç½‘ç«™URL
    benign_urls = []
    if (github_dir / "benign_urls.txt").exists():
        with open(github_dir / "benign_urls.txt", "r", encoding="utf-8") as f:
            benign_urls = [line.strip() for line in f if line.strip()]

    return phishing_urls, benign_urls

def save_validated_data(phishing_results: List[Dict], benign_results: List[Dict]):
    """ä¿å­˜éªŒè¯åçš„æ•°æ®"""
    output_dir = Path("validated_github_data")
    output_dir.mkdir(exist_ok=True)

    # åˆå¹¶æ‰€æœ‰ç»“æœ
    all_results = phishing_results + benign_results

    # è½¬æ¢ä¸ºDataFrame
    df = pd.DataFrame(all_results)

    # ä¿å­˜å®Œæ•´æ•°æ®
    df.to_parquet(output_dir / "validated_github_data.parquet", index=False)

    # åˆ†åˆ«ä¿å­˜æˆåŠŸå’Œå¤±è´¥çš„æ•°æ®
    success_df = df[df['success'] == True]
    failed_df = df[df['success'] == False]

    success_df.to_parquet(output_dir / "successful_urls.parquet", index=False)
    failed_df.to_parquet(output_dir / "failed_urls.parquet", index=False)

    # ä¿å­˜è®­ç»ƒæ•°æ®æ ¼å¼
    if not success_df.empty:
        training_data = []
        for _, row in success_df.iterrows():
            features = row.get('features', {})
            training_row = {
                'url': row['url'],
                'final_url': row['final_url'],
                'label': row['label'],
                'timestamp': row['timestamp']
            }
            # æ·»åŠ æ‰€æœ‰ç‰¹å¾
            training_row.update(features)
            training_data.append(training_row)

        training_df = pd.DataFrame(training_data)
        training_df.to_parquet(output_dir / "training_data.parquet", index=False)

    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total_phishing': len(phishing_results),
        'successful_phishing': len([r for r in phishing_results if r['success']]),
        'total_benign': len(benign_results),
        'successful_benign': len([r for r in benign_results if r['success']]),
        'total_urls': len(all_results),
        'successful_urls': len(success_df),
        'failed_urls': len(failed_df),
        'success_rate': len(success_df) / len(all_results) * 100 if all_results else 0
    }

    with open(output_dir / "validation_stats.json", "w", encoding="utf-8") as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ“Š éªŒè¯ç»Ÿè®¡:")
    print(f"  æ€»URLæ•°: {stats['total_urls']}")
    print(f"  æˆåŠŸéªŒè¯: {stats['successful_urls']}")
    print(f"  éªŒè¯å¤±è´¥: {stats['failed_urls']}")
    print(f"  æˆåŠŸç‡: {stats['success_rate']:.1f}%")
    print(f"  é’“é±¼ç½‘ç«™: {stats['successful_phishing']}/{stats['total_phishing']}")
    print(f"  è‰¯æ€§ç½‘ç«™: {stats['successful_benign']}/{stats['total_benign']}")

    return output_dir

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” å¼€å§‹éªŒè¯GitHubæ”¶é›†çš„URLå¹¶æå–ç‰¹å¾")
    print("=" * 60)

    # åŠ è½½æ•°æ®
    phishing_urls, benign_urls = load_github_data()

    print(f"ğŸ“¦ åŠ è½½æ•°æ®:")
    print(f"  é’“é±¼ç½‘ç«™: {len(phishing_urls)} ä¸ª")
    print(f"  è‰¯æ€§ç½‘ç«™: {len(benign_urls)} ä¸ª")

    # éªŒè¯é’“é±¼ç½‘ç«™
    print(f"\nğŸ¯ éªŒè¯é’“é±¼ç½‘ç«™...")
    phishing_results = await validate_and_extract_features(phishing_urls, label=1)

    # éªŒè¯è‰¯æ€§ç½‘ç«™
    print(f"\nâœ… éªŒè¯è‰¯æ€§ç½‘ç«™...")
    benign_results = await validate_and_extract_features(benign_urls, label=0)

    # ä¿å­˜æ•°æ®
    print(f"\nğŸ’¾ ä¿å­˜éªŒè¯åçš„æ•°æ®...")
    output_dir = save_validated_data(phishing_results, benign_results)

    print(f"\nğŸ‰ éªŒè¯å®Œæˆ! æ•°æ®å·²ä¿å­˜åˆ°: {output_dir}/")
    print("æ–‡ä»¶åˆ—è¡¨:")
    print("  - validated_github_data.parquet: å®Œæ•´éªŒè¯æ•°æ®")
    print("  - successful_urls.parquet: æˆåŠŸéªŒè¯çš„URL")
    print("  - failed_urls.parquet: éªŒè¯å¤±è´¥çš„URL")
    print("  - training_data.parquet: è®­ç»ƒæ•°æ®æ ¼å¼")
    print("  - validation_stats.json: éªŒè¯ç»Ÿè®¡ä¿¡æ¯")

if __name__ == "__main__":
    asyncio.run(main())